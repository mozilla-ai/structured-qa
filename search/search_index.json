{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Structured-QA Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Structured-QA Blueprint a local-first approach for answering questions about your structured documents.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Llama-cpp (question answering)</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-your-own-structured-qa-pipeline-in-minutes","title":"Start building your own Structured-QA pipeline in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor prompts and settings to adapt your use case.</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#structured_qa.preprocessing","title":"<code>structured_qa.preprocessing</code>","text":""},{"location":"api/#structured_qa.preprocessing.document_to_sections_dir","title":"<code>document_to_sections_dir(input_file, output_dir)</code>","text":"<p>Convert a document to a directory of sections.</p> <p>Uses pymupdf4llm to convert input_file to markdown. Then uses <code>split_markdown_by_headings</code> to split the markdown into sections based on the headers.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input document.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory. Structure of the output directory:</p> <pre><code>output_dir/\n    section_1.txt\n    section_2.txt\n    ...\n</code></pre> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of section names.</p> Source code in <code>src/structured_qa/preprocessing.py</code> <pre><code>@logger.catch(reraise=True)\ndef document_to_sections_dir(input_file: str, output_dir: str) -&gt; list[str]:\n    \"\"\"\n    Convert a document to a directory of sections.\n\n    Uses [pymupdf4llm](https://pypi.org/project/pymupdf4llm/) to convert input_file to markdown.\n    Then uses [`split_markdown_by_headings`][structured_qa.preprocessing.split_markdown_by_headings] to split the markdown into sections based on the headers.\n\n    Args:\n        input_file: Path to the input document.\n        output_dir: Path to the output directory.\n            Structure of the output directory:\n\n            ```\n            output_dir/\n                section_1.txt\n                section_2.txt\n                ...\n            ```\n\n    Returns:\n        List of section names.\n    \"\"\"\n\n    logger.info(f\"Converting {input_file}\")\n    md_text = pymupdf4llm.to_markdown(input_file)\n    Path(\"debug.md\").write_text(md_text)\n    logger.success(\"Converted\")\n\n    logger.info(\"Extracting sections\")\n    sections = split_markdown_by_headings(\n        md_text,\n    )\n    logger.success(f\"Found {len(sections)} sections\")\n    logger.info(f\"Writing sections to {output_dir}\")\n    output_dir = Path(output_dir)\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for section_name, section_content in sections.items():\n        (output_dir / f\"{section_name.replace('/', '_')}.txt\").write_text(\n            section_content\n        )\n    logger.success(\"Done\")\n\n    return sections.keys()\n</code></pre>"},{"location":"api/#structured_qa.preprocessing.split_markdown_by_headings","title":"<code>split_markdown_by_headings(markdown_text, heading_patterns=None)</code>","text":"<p>Splits a markdown document into sections based on specified heading patterns.</p> <p>Parameters:</p> Name Type Description Default <code>markdown_text</code> <code>str</code> <p>The markdown document as a single string.</p> required <code>heading_patterns</code> <code>str</code> <p>A list of regex patterns representing heading markers in the markdown document. Defaults to None. If None, the default patterns are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: A dictionary where the keys are the section names and the values are the section contents.</p> Source code in <code>src/structured_qa/preprocessing.py</code> <pre><code>def split_markdown_by_headings(\n    markdown_text, heading_patterns: list[str] | None = None\n) -&gt; dict[str, str]:\n    \"\"\"Splits a markdown document into sections based on specified heading patterns.\n\n    Args:\n        markdown_text (str): The markdown document as a single string.\n        heading_patterns (str, optional): A list of regex patterns representing heading markers\n            in the markdown document.\n            Defaults to None.\n            If None, the default patterns are used.\n\n    Returns:\n        dict[str, str]: A dictionary where the keys are the section names and the values are the section contents.\n    \"\"\"\n    if heading_patterns is None:\n        heading_patterns = [\n            r\"^#\\s+(.+)$\",\n            r\"^##\\s+(.+)$\",\n            r\"^###\\s+(.+)$\",\n            r\"^####\\s+(.+)$\",\n            r\"^\\*\\*[\\d\\.]+\\.\\*\\*\\s*\\*\\*(.+)\\*\\*$\",\n        ]\n\n    sections = defaultdict(str)\n\n    heading_text = \"INTRO\"\n    for line in markdown_text.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        for pattern in heading_patterns:\n            match = re.match(pattern, line)\n            if match:\n                heading_text = match.group(1)[:100]\n                break\n        sections[heading_text] += f\"{line}\\n\"\n\n    return sections\n</code></pre>"},{"location":"api/#structured_qa.model_loaders","title":"<code>structured_qa.model_loaders</code>","text":""},{"location":"api/#structured_qa.model_loaders.load_llama_cpp_model","title":"<code>load_llama_cpp_model(model_id)</code>","text":"<p>Loads the given model_id using Llama.from_pretrained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = load_llama_cpp_model(\"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <p>Returns:</p> Name Type Description <code>Llama</code> <code>LlamaModel</code> <p>The loaded model.</p> Source code in <code>src/structured_qa/model_loaders.py</code> <pre><code>def load_llama_cpp_model(model_id: str) -&gt; LlamaModel:\n    \"\"\"\n    Loads the given model_id using Llama.from_pretrained.\n\n    Examples:\n        &gt;&gt;&gt; model = load_llama_cpp_model(\"allenai/OLMoE-1B-7B-0924-Instruct-GGUF/olmoe-1b-7b-0924-instruct-q8_0.gguf\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n\n    Returns:\n        Llama: The loaded model.\n    \"\"\"\n    from llama_cpp import Llama\n\n    org, repo, filename = model_id.split(\"/\")\n    model = Llama.from_pretrained(\n        repo_id=f\"{org}/{repo}\",\n        filename=filename,\n        n_ctx=0,  # 0 means that the model limit will be used, instead of the default (512) or other hardcoded value\n        verbose=False,\n        n_gpu_layers=-1 if gpu_available() else 0,\n    )\n    return LlamaModel(model=model)\n</code></pre>"},{"location":"api/#structured_qa.workflow","title":"<code>structured_qa.workflow</code>","text":""},{"location":"api/#structured_qa.workflow.find_retrieve_answer","title":"<code>find_retrieve_answer(question, model, sections_dir, find_prompt, answer_prompt, max_sections_to_check=None)</code>","text":"<p>Workflow to find the relevant section, retrieve the information, and answer the question.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to answer.</p> required <code>model</code> <code>LlamaModel</code> <p>The model to use for generating completions.</p> required <code>sections_dir</code> <code>str</code> <p>The directory containing the sections. See <code>document_to_sections_dir</code>. Structure of the sections directory:</p> <pre><code>sections_dir/\n    section_1.txt\n    section_2.txt\n    ...\n</code></pre> required <code>find_prompt</code> <code>str</code> <p>The prompt for finding the section.</p> <p>See <code>FIND_PROMPT</code>.</p> required <code>answer_prompt</code> <code>str</code> <p>The prompt for answering the question.</p> <p>See <code>ANSWER_PROMPT</code>.</p> required <code>max_sections_to_check</code> <code>int</code> <p>The maximum number of sections to check before giving up. Defaults to None. If None, it will check  up to a maximum of 20 sections until it finds the answer.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, list[str]] | tuple[None, list[str]]</code> <p>tuple[str, list[str]] | tuple[None, list[str]]:</p> <p>If the answer is found, the tuple contains the answer and the sections checked. If the answer is not found, the tuple contains None and the sections checked</p> Source code in <code>src/structured_qa/workflow.py</code> <pre><code>def find_retrieve_answer(\n    question: str,\n    model: LlamaModel,\n    sections_dir: str,\n    find_prompt: str,\n    answer_prompt: str,\n    max_sections_to_check: int | None = None,\n) -&gt; tuple[str, list[str]] | tuple[None, list[str]]:\n    \"\"\"\n    Workflow to find the relevant section, retrieve the information, and answer the question.\n\n    Args:\n        question (str): The question to answer.\n        model (LlamaModel): The model to use for generating completions.\n        sections_dir (str): The directory containing the sections.\n            See [`document_to_sections_dir`][structured_qa.preprocessing.document_to_sections_dir].\n            Structure of the sections directory:\n\n            ```\n            sections_dir/\n                section_1.txt\n                section_2.txt\n                ...\n            ```\n        find_prompt (str): The prompt for finding the section.\n\n            See [`FIND_PROMPT`][structured_qa.config.FIND_PROMPT].\n        answer_prompt (str): The prompt for answering the question.\n\n            See [`ANSWER_PROMPT`][structured_qa.config.ANSWER_PROMPT].\n        max_sections_to_check (int, optional): The maximum number of sections to check before giving up.\n            Defaults to None.\n            If None, it will check  up to a maximum of 20 sections until it finds the answer.\n\n    Returns:\n        tuple[str, list[str]] | tuple[None, list[str]]:\n\n            If the answer is found, the tuple contains the answer and the sections checked.\n            If the answer is not found, the tuple contains None and the sections checked\n    \"\"\"\n    sections_dir = Path(sections_dir)\n    sections_names = [section.stem for section in sections_dir.glob(\"*.txt\")]\n    current_info = None\n    current_section = None\n\n    if max_sections_to_check is None:\n        max_sections_to_check = min(20, len(sections_names))\n\n    sections_checked = []\n    while len(sections_checked) &lt;= max_sections_to_check:\n        logger.debug(f\"Current information available: {current_info}\")\n        if not current_info:\n            logger.debug(\"Finding section\")\n            finding_section = True\n            question_part, *options = question.split(\"?\")\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": find_prompt.format(SECTIONS=\"\\n\".join(sections_names)),\n                },\n                {\"role\": \"user\", \"content\": question_part},\n            ]\n        else:\n            logger.debug(\"Answering question\")\n            finding_section = False\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": answer_prompt.format(CURRENT_INFO=current_info),\n                },\n                {\"role\": \"user\", \"content\": question},\n            ]\n\n        try:\n            response = model.get_response(messages)\n        except Exception as e:\n            logger.error(f\"Failed to generate completion: {e}\")\n            return \"Generation Error\", sections_checked\n\n        if finding_section:\n            response = response.strip()\n            if not sections_names:\n                return \"NOT FOUND\", sections_checked\n            section_name = get_matching_section(response, sections_names)\n            logger.debug(f\"Retrieving section: {section_name}\")\n            section_content = (sections_dir / f\"{section_name}.txt\").read_text()\n            current_section = section_name\n            current_info = section_content\n            sections_checked.append(section_name)\n\n        else:\n            if \"MORE INFO\" in response.upper():\n                current_info = None\n                sections_names.remove(current_section)\n                continue\n            else:\n                return response, sections_checked\n\n    return \"NOT FOUND\", sections_checked\n</code></pre>"},{"location":"api/#structured_qa.workflow.get_matching_section","title":"<code>get_matching_section(response, section_names)</code>","text":"<p>Use string similarity to find the most similar section_name.</p> Source code in <code>src/structured_qa/workflow.py</code> <pre><code>def get_matching_section(response, section_names):\n    \"\"\"\n    Use string similarity to find the most similar section_name.\n    \"\"\"\n    return process.extractOne(response, section_names)[0]\n</code></pre>"},{"location":"api/#structured_qa.config.FIND_PROMPT","title":"<code>structured_qa.config.FIND_PROMPT = '\\nYou are given two pieces of information:\\n1. A list of valid section names.\\n2. A user question.\\n\\nYour task is to:\\n- Identify exactly one `section_name` from the provided list that seems related to the user question.\\n- Return the `section_name` exactly as it appears in the list.\\n- Do NOT answer the question.\\n- Do NOT return any additional text, explanation, or formatting.\\n- Do NOT combine multiple section names into a single response.\\n\\nHere is the list of valid section names:\\n\\n```\\n{SECTIONS}\\n```\\n\\nNow, based on the following question, return the single most relevant `section_name` from the list.\\n'</code>  <code>module-attribute</code>","text":""},{"location":"api/#structured_qa.config.ANSWER_PROMPT","title":"<code>structured_qa.config.ANSWER_PROMPT = '\\nYou are a rigorous assistant answering questions.\\nYou must only answer based on the current information available which is:\\n\\n```\\n{CURRENT_INFO}\\n```\\n\\nIf the current information available not enough to answer the question,\\nyou must return \"I need more info\" and nothing else.\\n\\nIf the current information is enough to answer, you must return one of the following formats:\\n- YES/NO (for boolean questions)\\n- Number (for numeric questions)\\n- Single letter (for multiple-choice questions)\\n'</code>  <code>module-attribute</code>","text":""},{"location":"cli/","title":"Command Line Interface","text":"<p>Once you have installed the blueprint, you can use it from the CLI.</p> <p>You can either provide the path to a configuration file:</p> <pre><code>structured-qa --from_config \"example_data/config.yaml\"\n</code></pre> <p>Or provide values to the arguments directly:</p> <pre><code>structured-qa \\\n--question \"What learning rate was used?\" \\\n--input_file \"example_data/1706.03762v7.pdf\" \\\n--output_folder \"example_outputs/1706.03762v7.pdf\"\n</code></pre>"},{"location":"cli/#structured_qa.cli.structured_qa","title":"<code>structured_qa.cli.structured_qa(question, input_file=None, output_dir=None, model='bartowski/Qwen2.5-3B-Instruct-GGUF/Qwen2.5-3B-Instruct-f16.gguf', find_prompt=FIND_PROMPT, answer_prompt=ANSWER_PROMPT, from_config=None)</code>","text":"<p>Structured Question Answering.</p> <p>Split the input document into sections and answer the question based on the sections.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str | None</code> <p>Path to the input document.</p> <code>None</code> <code>output_dir</code> <code>str | None</code> <p>Path to the output directory. Structure of the output directory:</p> <pre><code>output_dir/\n    section_1.txt\n    section_2.txt\n    ...\n</code></pre> <code>None</code> <code>model</code> <code>str | None</code> <p>Model identifier formatted as <code>owner/repo/file</code>. Must be hosted at the HuggingFace Hub in GGUF format.</p> <code>'bartowski/Qwen2.5-3B-Instruct-GGUF/Qwen2.5-3B-Instruct-f16.gguf'</code> <code>question</code> <code>str</code> <p>The question to answer.</p> required <code>find_prompt</code> <code>str</code> <p>The prompt for finding the section.</p> <p>See <code>FIND_PROMPT</code>.</p> <code>FIND_PROMPT</code> <code>answer_prompt</code> <code>str</code> <p>The prompt for answering the question.</p> <p>See <code>ANSWER_PROMPT</code>.</p> <code>ANSWER_PROMPT</code> <code>from_config</code> <code>str | None</code> <p>The path to the config file.</p> <p>If provided, all other arguments will be ignored.</p> <code>None</code> Source code in <code>src/structured_qa/cli.py</code> <pre><code>@logger.catch(reraise=True)\ndef structured_qa(\n    question: str,\n    input_file: str | None = None,\n    output_dir: str | None = None,\n    model: str\n    | None = \"bartowski/Qwen2.5-3B-Instruct-GGUF/Qwen2.5-3B-Instruct-f16.gguf\",\n    find_prompt: str = FIND_PROMPT,\n    answer_prompt: str = ANSWER_PROMPT,\n    from_config: str | None = None,\n):\n    \"\"\"\n    Structured Question Answering.\n\n    Split the input document into sections and answer the question based on the sections.\n\n    Args:\n        input_file: Path to the input document.\n        output_dir: Path to the output directory.\n            Structure of the output directory:\n\n            ```\n            output_dir/\n                section_1.txt\n                section_2.txt\n                ...\n            ```\n        model: Model identifier formatted as `owner/repo/file`.\n            Must be hosted at the HuggingFace Hub in GGUF format.\n        question: The question to answer.\n        find_prompt: The prompt for finding the section.\n\n            See [`FIND_PROMPT`][structured_qa.config.FIND_PROMPT].\n        answer_prompt: The prompt for answering the question.\n\n            See [`ANSWER_PROMPT`][structured_qa.config.ANSWER_PROMPT].\n        from_config: The path to the config file.\n\n            If provided, all other arguments will be ignored.\n    \"\"\"\n    if from_config:\n        raw_config = yaml.safe_load(Path(from_config).read_text())\n        Path(raw_config[\"output_dir\"]).mkdir(exist_ok=True, parents=True)\n        config = Config.model_validate(raw_config)\n    else:\n        Path(output_dir).mkdir(exist_ok=True, parents=True)\n        config = Config(\n            input_file=input_file,\n            output_dir=output_dir,\n            model=model,\n            find_prompt=find_prompt,\n            answer_prompt=answer_prompt,\n        )\n\n    logger.info(\"Loading and converting to sections\")\n    document_to_sections_dir(config.input_file, config.output_dir)\n    logger.success(\"Done\")\n\n    logger.info(\"Loading Model\")\n    model = load_llama_cpp_model(config.model)\n    logger.success(\"Done\")\n\n    logger.info(\"Answering\")\n    answer, sections_checked = find_retrieve_answer(\n        model=model,\n        sections_dir=config.output_dir,\n        question=question,\n        find_prompt=config.find_prompt,\n        answer_prompt=config.answer_prompt,\n    )\n    logger.success(\"Done\")\n\n    logger.info(\"Sections checked:\")\n    logger.info(sections_checked)\n    logger.info(\"Answer:\")\n    logger.info(answer)\n</code></pre>"},{"location":"cli/#structured_qa.config.Config","title":"<code>structured_qa.config.Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/structured_qa/config.py</code> <pre><code>class Config(BaseModel):\n    input_file: FilePath\n    output_dir: DirectoryPath\n    model: Annotated[str, AfterValidator(validate_model)]\n    answer_prompt: Annotated[str, AfterValidator(answer_prompt)]\n    find_prompt: Annotated[str, AfterValidator(validate_find_prompt)]\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-model","title":"\ud83e\udde0 Changing the Model","text":""},{"location":"customization/#modifying-the-system-prompt","title":"\ud83d\udcdd Modifying the system Prompt","text":""},{"location":"customization/#other-customization-ideas","title":"\ud83d\udca1 Other Customization Ideas","text":"<ul> <li>other ideas..</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Issues.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started with Structured-QA using one of the options below:</p>"},{"location":"getting-started/#setup-options","title":"Setup options","text":"\u2601\ufe0f Google Colab (GPU)\u2601\ufe0f GitHub Codespaces\ud83d\udcbb Local Installation <p>The easiest way to play with the code on a GPU, for free.</p> <p>Click the button below to launch the project directly in Google Colab:</p> <p><p></p></p> <p>Click the button below to launch the project directly in GitHub Codespaces:</p> <p><p></p></p> <p>Once the Codespaces environment launches, inside the terminal, start the Streamlit demo by running:</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre> <p>You can install the project from Pypi:</p> <pre><code>pip install structured-qa\n</code></pre> <p>Check the Command Line Interface guide.</p> <p>Alternatively, you can clone and install it in editable mode:</p> <ol> <li> <p>Clone the Repository</p> <pre><code>git clone https://github.com/mozilla-ai/structured-qa.git\ncd structured-qa\n</code></pre> </li> <li> <p>Install the project and its Dependencies</p> <pre><code>pip install -e .\n</code></pre> </li> <li> <p>Run the Demo</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre> </li> </ol>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Structured-QA Blueprint Works","text":""},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This system has the following core stages:</p> <p>\ud83d\udcd1 1. Document Pre-Processing    Prepare the input document by extracting the different sections that compose the structure of the document.    Split the sections and save them to separate files.</p> <p>\ud83d\udd0e 2. Find Relevant and Retrieve Section    Given a list of sections and the input question, use the LLM to identify the section that looks more relevant. Load the individual section to be passed to the next step.</p> <p>\ud83d\udcd7 3. Answer Question    Use the LLM to answer the question based on the information available in the retrieved section.</p> <p>In case the LLM can't find an answer to the question, the stages 2 to 3 run on a loop until the LLM finds the answer.</p>"},{"location":"step-by-step-guide/#document-pre-processing","title":"Document Pre-Processing","text":"<p>The process begins with preparing the input document for AI processing. The document is first converted to markdown and then split into sections based on the markdown headings.</p> <p>Markdown Conversion</p> <ul> <li>Uses pymupdf4llm</li> </ul> <p>Section Splitting</p> <ul> <li> <p>Uses split_markdown_by_headings</p> </li> <li> <p>Each section is saved to a separate file.</p> </li> </ul>"},{"location":"step-by-step-guide/#api-example","title":"\ud83d\udd0d API Example","text":"<pre><code>from structured_qa.preprocessing import document_to_sections_dir\n\nsection_names = document_to_sections_dir(\n    \"example_data/1706.03762v7.pdf\", \"example_outputs/1706.03762v7\"\n)\nprint(section_names)\n\"\"\"\n['attention is all you need', '1 introduction', '2 background', '3 model architecture', '4 why self-attention', '5 training', '6 results', '7 conclusion', 'references', 'attention visualizations']\n\"\"\"\n</code></pre>"},{"location":"step-by-step-guide/#find-retrieve-answer","title":"Find, Retrieve, Answer","text":"<p>These steps run in a loop until an answer is found or the maximum number of iterations is reached. An input <code>model</code> previously loaded with <code>load_llama_cpp_model</code> must be provided.</p> <p>The loop is defined in <code>find_retrieve_answer</code></p> <p>Find</p> <ul> <li>Using the <code>section_names</code> from the pre-processing, calls the <code>model</code> with the <code>FIND_PROMPT</code></li> </ul> <p>Retrieve</p> <ul> <li>Loads the <code>section</code> file picked by the <code>model</code>.</li> </ul> <p>Answer</p> <ul> <li>Calls the <code>model</code> with the <code>ANSWER_PROMPT</code></li> </ul>"},{"location":"step-by-step-guide/#api-example_1","title":"\ud83d\udd0d API Example","text":"<pre><code>from structured_qa.config import ANSWER_PROMPT, FIND_PROMPT\nfrom structured_qa.model_loaders import load_llama_cpp_model\nfrom structured_qa.workflow import find_retrieve_answer\n\n# Load the model\nmodel = load_llama_cpp_model(\n    \"bartowski/Qwen2.5-3B-Instruct-GGUF/Qwen2.5-3B-Instruct-f16.gguf\"\n)\n\nanswer, sections_checked = find_retrieve_answer(\n    question=\"What optimizer was using for training?\",\n    model=model,\n    sections_dir=\"example_outputs/1706.03762v7\",\n    find_prompt=FIND_PROMPT,\n    answer_prompt=ANSWER_PROMPT\n)\nprint(answer)\n\"\"\"\nThe optimizer used during training was Adam, with parameters \u03b21 = 0.9, \u03b22 = 0.98, and \u03f5 = 10^\u22129.\n\"\"\"\nprint(sections_checked)\n\"\"\"\n['5 training']\n\"\"\"\n</code></pre>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}