{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code: https://github.com/mozilla-ai/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://mozilla-ai.github.io/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to `Edit`â†’`Notebook Settings`\n",
    "- Select T4 GPU from the Hardware Accelerator section\n",
    "- Click `Save` and accept.\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU not available\")\n",
    "else:\n",
    "    print(\"GPU is available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Download Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def download_document(url, output_file):\n",
    "    if not Path(output_file).exists():\n",
    "        urlretrieve(url, output_file)\n",
    "        logger.info(f\"Downloaded {url} to {output_file}\")\n",
    "    else:\n",
    "        logger.info(f\"File {output_file} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Process a single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "ANSWER_WITH_TYPE_PROMPT = \"\"\"\n",
    "You are a rigorous assistant answering questions.\n",
    "You only answer based on the current information available.\n",
    "You should only answer with ANSWER_TYPE.\n",
    "\n",
    "The current information available is:\n",
    "\n",
    "{CURRENT_INFO}\n",
    "\n",
    "If the current information available not enough to answer the question,\n",
    "you must return the following message and nothing else:\n",
    "\n",
    "```\n",
    "I need more info.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def process_document(\n",
    "    document_file,\n",
    "    document_data,\n",
    "    model,\n",
    "    answer_prompt=ANSWER_WITH_TYPE_PROMPT,\n",
    "):\n",
    "    logger.info(\"Setting up RAG\")\n",
    "    RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "    RAG.index([document_file])\n",
    "\n",
    "    logger.info(\"Predicting\")\n",
    "    answers = {}\n",
    "    sections = {}\n",
    "    for index, row in document_data.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        try:\n",
    "            float(row[\"answer\"])\n",
    "            answer_type = \"a number\"\n",
    "        except ValueError:\n",
    "            if row[\"answer\"] in (\"YES\", \"NO\"):\n",
    "                answer_type = \"YES or NO\"\n",
    "            else:\n",
    "                answer_type = \"a single letter\"\n",
    "\n",
    "            answer_prompt = answer_prompt.replace(\"ANSWER_TYPE\", answer_type)\n",
    "\n",
    "        logger.info(f\"Question: {question}\")\n",
    "        logger.info(\"RAG search\")\n",
    "        results = RAG.search(query=question, k=3)\n",
    "\n",
    "        current_info = \"\\n\".join(result[\"content\"] for result in results)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": answer_prompt.format(CURRENT_INFO=current_info),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        answer = model.get_response(messages)\n",
    "\n",
    "        answers[index] = answer\n",
    "        sections[index] = None\n",
    "\n",
    "    return answers, sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structured_qa.model_loaders import load_llama_cpp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_llama_cpp_model(\n",
    "    \"bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logger.info(\"Loading input data\")\n",
    "data = pd.read_csv(\"structured_qa.csv\")\n",
    "data[\"pred_answer\"] = [None] * len(data)\n",
    "data[\"pred_section\"] = [None] * len(data)\n",
    "\n",
    "for document_link, document_data in data.groupby(\"document\"):\n",
    "    logger.info(f\"Downloading document {document_link}\")\n",
    "    downloaded_document = Path(f\"{Path(document_link).name}.pdf\")\n",
    "    download_document(document_link, downloaded_document)\n",
    "\n",
    "    answers, sections = process_document(downloaded_document, document_data, model)\n",
    "\n",
    "    for index in document_data.index:\n",
    "        data.loc[index, \"pred_answer\"] = str(answers[index]).upper()\n",
    "        data.loc[index, \"pred_section\"] = sections[index]\n",
    "\n",
    "data.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results.csv\")\n",
    "results.loc[results[\"answer\"] != results[\"pred_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(results[\"answer\"] == results[\"pred_answer\"]) / len(results)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
