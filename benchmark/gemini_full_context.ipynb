{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code: https://github.com/mozilla-ai/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs: https://mozilla-ai.github.io/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"INFO\"\n",
    "genai.configure(api_key=\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structured_qa.model_loaders import load_gemini_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Process a single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def process_document(\n",
    "    document_file,\n",
    "    document_data,\n",
    "    model,\n",
    "):\n",
    "    logger.info(\"Uploading file\")\n",
    "    file = genai.upload_file(document_file, mime_type=\"application/pdf\")\n",
    "    while file.state.name == \"PROCESSING\":\n",
    "        logger.debug(\"Waiting for file to be processed.\")\n",
    "        time.sleep(2)\n",
    "        file = genai.get_file(file.name)\n",
    "\n",
    "    logger.info(\"Predicting\")\n",
    "    n = 0\n",
    "    answers = {}\n",
    "    sections = {}\n",
    "    for index, row in document_data.iterrows():\n",
    "        if n > 0 and n % 9 == 0:\n",
    "            logger.info(\"Waiting for 60 seconds\")\n",
    "            time.sleep(60)\n",
    "        question = row[\"question\"]\n",
    "        logger.debug(f\"Question: {question}\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    file,\n",
    "                    question,\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        response = model.get_response(messages)\n",
    "        logger.debug(response)\n",
    "        response_json = json.loads(response)\n",
    "        answers[index] = response_json[\"answer\"]\n",
    "        sections[index] = response_json[\"section\"]\n",
    "        n += 1\n",
    "    return answers, sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Download Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def download_document(url, output_file):\n",
    "    if not Path(output_file).exists():\n",
    "        urlretrieve(url, output_file)\n",
    "        logger.debug(f\"Downloaded {url} to {output_file}\")\n",
    "    else:\n",
    "        logger.debug(f\"File {output_file} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_CONTEXT_PROMPT = \"\"\"\n",
    "You are given an input document and a question.\n",
    "You can only answer the question based on the information in the document.\n",
    "You will return a JSON name with two keys: \"section\" and \"answer\".\n",
    "In `\"section\"`, you will return the name of the section where you found the answer.\n",
    "In `\"answer\"`, you will return the answer one of the following JSON:\n",
    "- Yes/No (for boolean questions)\n",
    "Is the model an LLM?\n",
    "{\n",
    "  \"section\": \"1. Introduction\",\n",
    "  \"answer\": \"No\"\n",
    "}\n",
    "- Single number (for numeric questions)\n",
    "How many layers does the model have?\n",
    "{\n",
    "  \"section\": \"2. Architecture\",\n",
    "  \"answer\": 12\n",
    "}\n",
    "- Single letter (for multiple-choice questions)\n",
    "What is the activation function used in the model?\n",
    "-A: ReLU\n",
    "-B: Sigmoid\n",
    "-C: Tanh\n",
    "{\n",
    "  \"section\": \"2. Architecture\",\n",
    "  \"answer\": \"C\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_gemini_model(\n",
    "    \"gemini-2.0-flash-exp\",\n",
    "    system_prompt=FULL_CONTEXT_PROMPT,\n",
    "    generation_config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "logger.info(\"Loading input data\")\n",
    "data = pd.read_csv(\"structured_qa.csv\")\n",
    "data[\"pred_answer\"] = [None] * len(data)\n",
    "data[\"pred_section\"] = [None] * len(data)\n",
    "\n",
    "for document_link, document_data in data.groupby(\"document\"):\n",
    "    logger.info(f\"Downloading document {document_link}\")\n",
    "    downloaded_document = Path(f\"{Path(document_link).name}.pdf\")\n",
    "    download_document(document_link, downloaded_document)\n",
    "\n",
    "    answers, sections = process_document(downloaded_document, document_data, model)\n",
    "\n",
    "    for index in document_data.index:\n",
    "        data.loc[index, \"pred_answer\"] = str(answers[index]).upper()\n",
    "        data.loc[index, \"pred_section\"] = sections[index]\n",
    "\n",
    "data.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results.csv\")\n",
    "results.loc[results[\"answer\"] != results[\"pred_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(results[\"answer\"] == results[\"pred_answer\"]) / len(results)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
