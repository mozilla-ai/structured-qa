{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcx4osZYq3mt"
      },
      "source": [
        "# Structured Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE32hJKeq3mv"
      },
      "source": [
        "Source code: https://github.com/mozilla-ai/structured-qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDIEL7SNq3mv"
      },
      "source": [
        "Docs: https://mozilla-ai.github.io/structured-qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OwS4mKRq3mv"
      },
      "source": [
        "## GPU Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FYZaTmnq3mw"
      },
      "source": [
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to `Edit`→`Notebook Settings`\n",
        "- Select T4 GPU from the Hardware Accelerator section\n",
        "- Click `Save` and accept.\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RsETkxfq3mw",
        "outputId": "72974d48-d8d5-4ad4-e247-801248849940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPU not available\")\n",
        "else:\n",
        "    print(\"GPU is available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEgVEmSQq3mx"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1eAychVq3my",
        "outputId": "088fe47d-7bf6-42c9-f538-e0052369aff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille in /usr/local/lib/python3.11/dist-packages (0.0.8.post4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: colbert-ai==0.2.19 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.2.19)\n",
            "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (1.10.0)\n",
            "Requirement already satisfied: fast-pytorch-kmeans==0.2.0.1 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.2.0.1)\n",
            "Requirement already satisfied: langchain>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.16)\n",
            "Requirement already satisfied: langchain_core>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.32)\n",
            "Requirement already satisfied: llama-index>=0.7 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.12.15)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.7.0)\n",
            "Requirement already satisfied: srsly==2.4.8 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.4.8)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (4.47.1)\n",
            "Requirement already satisfied: voyager<3.0.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.1.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (3.2.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (3.1.0)\n",
            "Requirement already satisfied: git-python in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.0.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.11.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.67.1)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (5.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (12.0.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (4.12.2)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.4.3)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.15 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.12.15)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.6.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.3.17)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.4.4)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.18.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core>=0.1.4->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.23.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.59.9)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (0.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index>=0.7->ragatouille) (0.1.11)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (5.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.7->ragatouille) (0.5.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (1.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->ragatouille) (3.1.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->ragatouille) (3.0.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.44)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->fast-pytorch-kmeans==0.2.0.1->ragatouille) (12.570.86)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.15->llama-index>=0.7->ragatouille) (3.26.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install ragatouille PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0dl5xGnq3my",
        "outputId": "8b3f9944-46f3-423e-e5f4-7738edec5966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark\n",
            "  Cloning https://github.com/mozilla-ai/structured-qa.git (to revision 5-add-benchmark) to /tmp/pip-req-build-v6q9_weu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mozilla-ai/structured-qa.git /tmp/pip-req-build-v6q9_weu\n",
            "  Running command git checkout -b 5-add-benchmark --track origin/5-add-benchmark\n",
            "  Switched to a new branch '5-add-benchmark'\n",
            "  Branch '5-add-benchmark' set up to track remote branch '5-add-benchmark' from 'origin'.\n",
            "  Resolved https://github.com/mozilla-ai/structured-qa.git to commit 97049d67d83ec6129569d442bd365c7a5e490578\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (0.7.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (0.27.1)\n",
            "Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (0.7.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (2.10.6)\n",
            "Requirement already satisfied: pymupdf4llm in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (0.0.17)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (6.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (3.12.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev111+g97049d6) (1.41.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->structured-qa==0.3.3.dev111+g97049d6) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev111+g97049d6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev111+g97049d6) (2.27.2)\n",
            "Requirement already satisfied: pymupdf>=1.24.10 in /usr/local/lib/python3.11/dist-packages (from pymupdf4llm->structured-qa==0.3.3.dev111+g97049d6) (1.25.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (1.26.4)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev111+g97049d6) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (1.24.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev111+g97049d6) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev111+g97049d6) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev111+g97049d6) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev111+g97049d6) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl_haxghq3mz",
        "outputId": "773b3a25-285b-408a-bc9d-f490576f91fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-04 15:27:16--  https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23304 (23K) [text/plain]\n",
            "Saving to: ‘structured_qa.csv.1’\n",
            "\n",
            "structured_qa.csv.1 100%[===================>]  22.76K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-02-04 15:27:17 (28.5 MB/s) - ‘structured_qa.csv.1’ saved [23304/23304]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdWx_e7iq3mz"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vGqX_bU5q3mz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab.userdata import get, SecretNotFoundError\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=get(\"GOOGLE_API_KEY\"))\n",
        "except SecretNotFoundError as e:\n",
        "    raise RuntimeError(\"Please set the GOOGLE_API_KEY secret to your API key\") from e\n",
        "os.environ[\"LOGURU_LEVEL\"] = \"INFO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cbkIjBYNq3mz"
      },
      "outputs": [],
      "source": [
        "from loguru import logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BiUeBWnIq3mz"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "\n",
        "def load_pdf(pdf_file: str) -> str | None:\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        return \"\\n\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0B2yhFISDgG"
      },
      "source": [
        "## Function to Process all questions for a single Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ilxn8LGFq3m0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.data import CorpusProcessor\n",
        "\n",
        "\n",
        "def process_document(\n",
        "    document_file,\n",
        "    document_data,\n",
        "    model,\n",
        "):\n",
        "    logger.info(\"Setting up RAG\")\n",
        "    RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    corpus_processor = CorpusProcessor()\n",
        "    documents = corpus_processor.process_corpus([load_pdf(document_file)])\n",
        "    RAG.encode([x[\"content\"] for x in documents])\n",
        "\n",
        "    logger.info(\"Predicting\")\n",
        "    answers = {}\n",
        "    sections = {}\n",
        "    for index, row in document_data.iterrows():\n",
        "        if model.n > 0 and model.n % 9 == 0:\n",
        "            logger.info(\"Waiting for 60 seconds\")\n",
        "            time.sleep(60)\n",
        "        question = row[\"question\"]\n",
        "        question_part, *options = question.split(\"?\")\n",
        "\n",
        "        logger.info(f\"Question: {question}\")\n",
        "        results = RAG.search_encoded_docs(query=question_part, k=3)\n",
        "        current_info = \"\\n\".join(result[\"content\"] for result in results)\n",
        "        logger.info(current_info[:100])\n",
        "\n",
        "        answer = model.model.generate_content(\n",
        "            [f\"This is the document: {current_info}\", question]\n",
        "        )\n",
        "        logger.info(answer.text)\n",
        "        answers[index] = answer.text.strip()\n",
        "        sections[index] = None\n",
        "        model.n += 1\n",
        "\n",
        "    return answers, sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3ke2aJq3m0"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zKMHc0Ouq3m0"
      },
      "outputs": [],
      "source": [
        "from structured_qa.model_loaders import load_gemini_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cMBl2dxLq3m0"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a rigorous assistant answering questions.\n",
        "You must only answer based on the current information available which is:\n",
        "\n",
        "```\n",
        "{CURRENT_INFO}\n",
        "```\n",
        "\n",
        "If the current information available not enough to answer the question,\n",
        "you must return \"I need more info\" srting and nothing else:\n",
        "\n",
        "If the current information is enough to answer, you must return one of the following formats:\n",
        "- YES/NO (for boolean questions)\n",
        "- Number (for numeric questions)\n",
        "- Single letter (for multiple-choice questions)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QV3pBXvhq3m0"
      },
      "outputs": [],
      "source": [
        "model = load_gemini_model(\"gemini-2.0-flash-exp\", system_prompt=SYSTEM_PROMPT)\n",
        "model.n = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5jWlVBaq3m1"
      },
      "source": [
        "# Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fcbeac06bbec4921b0c45d61e1e89b88",
            "153cbeb65e6f401aac9648d8b11046ff",
            "f0972bb4d4634bab9cda10b2baa31a71",
            "0803dde3898641b2a8f79ab88eb0653f",
            "e26c645517e54af1ade0e052394e9628",
            "f06f8db65bac463c98ecb34f2dcb0dfb",
            "3415497210a047d78a24086b9f88f91c",
            "ee7f2b57a99c4189b1429db4e6bce3fb",
            "98ff4a5a268a458da6a7206ed3e437d7",
            "70a80cb1055d4786aa1f8a84ef3157ce",
            "794d0b6347ab45b6ac7bf2d7eeb630e4",
            "6b38d9d92f09428a9d9670f918599e81",
            "e14603a547e8473fa824cc7685b62456",
            "fab6bb0faa174a4aa02366b037660fa0",
            "b4832b95793c4d308d4d662c031f0b51",
            "5bab6e216cdd4fd7a1d514d91056f275",
            "95e5af6381ee424e92e17622294ce455",
            "b38ffca43a2942c78216e663d3712dc2",
            "b232acd3b4f240e8b9815454dd78b64d",
            "0b4c805c2daf4e7dafbd6aedc458b4ef",
            "9cb7af363f1a4372aec5826e2dc4d7e8",
            "1b59bbf7535945c2b290f4068e02214d",
            "c5031f4a757d43ec87fc4dd068773aa4",
            "11e03bd58cf04306aaaa1ddaf9b30c38",
            "006194e2d15246c8ac0006f4ad0ec3d5",
            "6c0db984c21f4b668af8895961c74a91",
            "b02f570cc69f4da78aad558e9d1c3502",
            "2f8c4648e71d4297ab315f8f84814d16",
            "fd9a1d362a2c4aa4a9217b7913aab50b",
            "52896569fd8c47a58f7f1e04dc2982ad",
            "7d60dbc988c547168f59fded596f6326",
            "91c6634d9446477caddf5bc0baf79b74",
            "481d046b1c2c427abea771e6a900be4b",
            "46246ea8cd134dcb952f2009bed30ff5",
            "c55bbfdc199749a2b2c4e6514b96af71",
            "228881dd188447f89f6d5a5e30ba2b39",
            "e8fd851273e14870824433d33f71d521",
            "69428342766b4ec28848d8cce5c88328",
            "b280d7fd36e243f49a55ea5c90b3d96b",
            "8d226412b4e749839f8ed0efc23f3456",
            "858afce4603c4ac4ad6ae8a9817adfc6",
            "fcf8e5b3abed4bd49df30e3235bbbc5d",
            "b789dd69092449478667563eb96ae51b",
            "2df45b5f52884ad689c0f7da99f9046d",
            "d448a6c57d454df3a40509bebc92d337",
            "ff4a3e4282164dbcbd30ad0123ce9966",
            "3570f41c24a04d039f6e92f9c8e57cc3",
            "856f08461e6d4d218306310f38dc00ed",
            "1309b4cac445449ca766b881004dd04c",
            "9761af9a6ca14a03a6313fbc4edba9c7",
            "de42f1287b0b42f59efe335d8d96cebe",
            "6fafe7befa5e46d2890e63327b846172",
            "80d6d834e29140a7b452634cba9223e7",
            "d25ed47d1fdf402db19dd3736e79a9cd",
            "429b6a5553c24b31acb8d7a19f947d63",
            "42489b00de754d8ebc183a12f649eb8e",
            "f2404b6235374639bbf22cfdc7f2dd84",
            "86b48915fae748eeb3a572e793f8f8c7",
            "fc2dcb10a10244b7b19ca3163c0d5b17",
            "03078427cd344cd9bdc5e18da8a6dfa4",
            "7220498a1cb44256b294c639e016b80e",
            "ea5a5b11334243048936cd96d7189348",
            "add5098a768f4b8292919aad0f7a95be",
            "4315a7426a7c4bf7a295ed915cb7af0e",
            "9b1e49d8f8444ced936dee33f632dd83",
            "9c2f76016b7b4e819e3cac5bb8afa2b3",
            "a4f77aec2ec947508a6430f99df88cb4",
            "050caa495ff1470e9ccbb69e337edf41",
            "a000ee3a60924a73ae9a07605848924c",
            "b1392a03cdb045c2833aaf5370ba9879",
            "ef6d766fc5304b34b14ce2722570b785",
            "1e141e9becd044fbab6394748b9838a2",
            "99a6d80264534dbdb40563b876fad6ba",
            "5210b33e6f5547bd8689c50506d20949",
            "2efd2ec934cb485ebb222ba39ef9b5c6",
            "6e25ac32d79340ff9714bc02aaa5f95a",
            "9459c2d1de6d420cb621a139141f429a"
          ]
        },
        "id": "W9r17Rz3q3m1",
        "outputId": "5ffcccdc-9569-4ec4-c14e-204994917725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:27:45.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoading input data\u001b[0m\n",
            "\u001b[32m2025-02-04 15:27:45.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:27:45.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf to HAI_AI-Index-Report-2024.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:27:45.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcbeac06bbec4921b0c45d61e1e89b88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b38d9d92f09428a9d9670f918599e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5031f4a757d43ec87fc4dd068773aa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46246ea8cd134dcb952f2009bed30ff5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d448a6c57d454df3a40509bebc92d337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42489b00de754d8ebc183a12f649eb8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4f77aec2ec947508a6430f99df88cb4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 1214 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/38 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 38/38 [00:05<00:00,  6.52it/s]\n",
            "\u001b[32m2025-02-04 15:28:42.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:42.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: which type of risk was identified as the leading concern globally? -A: Fairness risks. -B: Privacy and data governance risks. -C: Risks related to generative AI deployment.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([1214, 508, 128])\n",
            "doc_masks: torch.Size([1214, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:28:43.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNotably, they observe that these concerns are \n",
            "significantly higher in Asia and Europe compared to \n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:44.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:44.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In which geographical area were fairness risks selected by the smallest percentage of respondents? -A: Asia. -B: Europe. -C: North America.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:44.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNotably, they observe that these concerns are \n",
            "significantly higher in Asia and Europe compared to \n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:45.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:45.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is a major consequence of the rising training costs for foundation models? -A: The exclusion of universities from developing leading-edge foundation models. -B: Increased collaboration between universities and AI companies. -C: A decrease in the number of policy initiatives related to AI research.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:45.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile AI \n",
            "companies seldom reveal the expenses involved \n",
            "in training their models, it is widely beli\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:46.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:46.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How the AI Index and Epoch AI estimated training costs for foundation models? -A: By surveying AI companies on their reported expenses. -B: By analyzing government funding allocated to AI research. -C: By analyzing training duration, hardware type, quantity, and utilization rate.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:46.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mUnderstanding the cost of training AI models is \n",
            "important, yet detailed information on these costs \u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:48.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:48.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is a major source of inequality in AI related to tokenization? -A: The significant variability in the number of tokens required to represent the same content across different languages. -B: The uniform processing speed of all languages. -C: The consistent cost of inference across different languages.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:48.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mthe word “dog”) which can \n",
            "help the model to make a better prediction (“fetch” vs \n",
            "“baseball”). If m\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:49.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:49.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What are the three major inequalities resulting from variable tokenization? -A: Increased model training costs, limited access to resources, and biased results. -B: Higher inference costs, longer processing times, and reduced available context for the model. -C:  Limited language support, increased hardware requirements, and data bias.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:49.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor \n",
            "instance, Portuguese closely matches English in the \n",
            "efficiency of the GPT-4 tokenizer, yet it \u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:50.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:50.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many AI-related regulations were enacted in the United States in 2023?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:50.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn 2023, there were 25 AI-related \n",
            "regulations, up from just one in 2016. Last year alone, the total\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:51.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m25\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:51.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which of the following was identified as a high relevance AI regulation? -A: Securities and Exchange Commission’s Cybersecurity Risk Management Strategy. -B: Copyright Office and Library of Congress’ Copyright Registration Guidance. -C: Regulations related to foreign trade and international finance\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:51.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA medium relevance regulation includes meaningful mentions of AI but is not solely centered on it. A\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:53.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:53.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which country had the highest proportion of female bachelor's graduates in informatics, computer science, computer engineering, and information technology among the surveyed European nations? -A: France. -B: Bulgaria. -C: United Kingdom\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:53.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCS, CE, and IT Bachelor’s \n",
            "Graduates\n",
            "In 2022, the United Kingdom led with the highest \n",
            "number of new\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:54.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:28:54.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:54.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which countries reported the smallest proportion of female master's graduates in informatics, CS, CE, and IT as of 2022? -A: Estonia, Romania, and Bulgaria. -B: United Kingdom, Germany, and Switzerland. -C: Belgium, Italy, and Switzerland.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:54.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mDespite some narrowing since 2011, men continue to \n",
            "dominate. For example, France (14.8%), the Unite\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:56.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:56.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/1706.03762\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:56.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/1706.03762 to 1706.03762.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:56.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 56 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00,  7.91it/s]\n",
            "\u001b[32m2025-02-04 15:29:59.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:59.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What type of architecture does the model use? -A: decoder only -B: encoder only -C: encoder-decoder\u001b[0m\n",
            "\u001b[32m2025-02-04 15:29:59.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mAt each step the model is auto-regressive\n",
            "[10], consuming the previously generated symbols as additi\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([56, 508, 128])\n",
            "doc_masks: torch.Size([56, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:30:00.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:00.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers compose the encoder?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:00.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mAt each step the model is auto-regressive\n",
            "[10], consuming the previously generated symbols as additi\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:01.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m6\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:01.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers compose the decoder?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:01.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "laye\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:02.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m6\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:02.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many parallel attention heads are used?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:02.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmod\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:04.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m8\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:04.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the final model use learned embeddings for the input and output tokens?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:04.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in o\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:05.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:05.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the final model use learned positional embeddings?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:05.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTo this end, we add \"positional encodings\" to the input embeddings at the\n",
            "bottoms of the encoder and\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:06.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:06.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many GPUs were used for training?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:06.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
            "batch contained a\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:07.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m8\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:07.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What type of GPUs were used for training? -A: NVIDIA A100 -B: NVIDIA P100 -C: NVIDIA T4\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:07.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
            "batch contained a\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:08.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:30:08.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:08.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What optimizer was used for training? -A: AdamW -B: Adam -C: SGD\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:08.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
            "(3.5 days).\n",
            "5.3 Optimizer\n",
            "We used the Adam optimizer [\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:09.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:09.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many warmup steps were used?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:09.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
            "(3.5 days).\n",
            "5.3 Optimizer\n",
            "We used the Adam optimizer [\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:11.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m4000\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:11.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What was the dropout rate used for the base model?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:11.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m36 41.29 7.7·10191.2·1021\n",
            "Transformer (base model) 27.3 38.1 3.3·1018\n",
            "Transformer (big) 28.4 41.8 2.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:12.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m0.  1\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:12.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2106.09685.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:13.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2106.09685.pdf to 2106.09685.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:13.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 137 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 5/5 [00:00<00:00,  9.22it/s]\n",
            "\u001b[32m2025-02-04 15:31:22.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:22.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does LoRA work with any neural network containing dense layers?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:22.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mMore importantly, these method often fail to\n",
            "match the ﬁne-tuning baselines, posing a trade-off betw\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([137, 508, 128])\n",
            "doc_masks: torch.Size([137, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:31:23.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:23.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: By how much can LoRA reduce GPU memory requirements during training? -A: 10x, -B: 5x, -C: 3x\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:23.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mUsing GPT-3 175B as an example – deploying indepen-\n",
            "dent instances of ﬁne-tuned models, each with 17\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:24.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:24.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In billions, how many trainable parameters does GPT-3 have?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:24.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe results\n",
            "on WikiSQL have a ﬂuctuation around \u00060:5%, MNLI-m around \u00060:1%, and SAMSum around\n",
            "\u00060:2/\u0006\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:25.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m175\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:25.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does LoRA introduce additional inference latency compared to full fine-tuning?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:25.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\n",
            "a sub\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:27.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:27.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2201.11903\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:27.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2201.11903 to 2201.11903.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:27.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 199 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 7/7 [00:00<00:00,  7.52it/s]\n",
            "\u001b[32m2025-02-04 15:31:30.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:30.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is Arithmetic reasoning is a task that language models often find very easy?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:30.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
            "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([199, 508, 128])\n",
            "doc_masks: torch.Size([199, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:31:31.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:31.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many large language models were evaluated?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:31.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor AQuA, we used four exemplars\n",
            "and solutions from the training set, as given in Appendix Table 21.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:32.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mFive\u001b[0m\n",
            "\u001b[32m2025-02-04 15:31:32.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:32.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many benchmarks were used to evaluate arithmetic reasoning?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:32.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
            "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:33.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mFive\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:33.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is symbolic reasoning usually simple for humans but challenging for language models?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:33.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m2We sample examples \u001460tokens to ﬁt into our input context window, and also limit the examples to \u00142\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:35.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:35.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many words have the example names that the model has seen for letter concatenation? -A: 3 -B: 2 -C: 4\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:35.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoebe ﬂips the coin.\n",
            "Osvaldo does not ﬂip the coin. Is the coin still heads up?”\n",
            "!“no” ).\n",
            "As the co\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:36.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:36.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which symbolic reasoning task is used as an out-of-domain evaluation? -A: Coin Flip -B: Tower of Hanoi -C: Chess puzzles\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:36.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoebe ﬂips the coin.\n",
            "Osvaldo does not ﬂip the coin. Is the coin still heads up?”\n",
            "!“no” ).\n",
            "As the co\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:38.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:38.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many annotators provided independent chains of thought?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:38.375\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSimilar to the\n",
            "annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instruct\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:39.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mThree\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:39.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many random samples were examined to understand model performance?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:39.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWe also randomly examined 50 random sam-\n",
            "ples for which the model gave the wrong answer.\n",
            "The summary\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:40.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m50\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:40.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2210.05189\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:41.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2210.05189 to 2210.05189.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:41.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 44 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00,  8.95it/s]\n",
            "\u001b[32m2025-02-04 15:32:42.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:42.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can recurrent networks also be converted to decision trees?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:42.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, for continuous activations, the neural\n",
            "network equivalent tree immediately becomes inﬁnit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([44, 508, 128])\n",
            "doc_masks: torch.Size([44, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:32:43.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:43.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers are in the toy model (y = x^2)?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:43.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:45.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:45.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the toy model (y = x^2) use Sigmoid activation function?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:45.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:46.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:32:46.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:46.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many parameters are in the toy model (y = x^2) tree?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:46.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:48.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m14\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:48.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers are in the half-moon neural network?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:48.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne\n",
            "can clearly interpret and moreover make deduction from the\n",
            "decision tree, some of which are as f\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:49.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:49.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the main computational advantage of decision trees? -A: Less storage memory, -B: Fewer operations, -C: Lower accuracy\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:49.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThere are also\n",
            "some categories that emerged although none of the training\n",
            "data falls to them.\n",
            "Beside\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:50.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2302.13971\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:50.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2302.13971 to 2302.13971.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:50.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 144 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 5/5 [00:00<00:00,  8.78it/s]\n",
            "\u001b[32m2025-02-04 15:33:54.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:54.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What proportion of the pre-training data was from Github? -A: 4.5% -B: 15.0% -C: 4%\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:54.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFinally, we deduplicate the result-\n",
            "ing dataset at the ﬁle level, with exact matches.\n",
            "Wikipedia [4.5\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([144, 508, 128])\n",
            "doc_masks: torch.Size([144, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:33:55.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:55.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many languages did the Wikipedia data cover?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:55.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis process deduplicates the data at the\n",
            "line level, performs language identiﬁcation with\n",
            "a fastTex\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:56.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m20\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:56.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What optimizer was used for training? -A: AdamW -B: Adam -C: SGD\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:56.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTo improve the\n",
            "training stability, we normalize the input of each\n",
            "transformer sub-layer, instead of \u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:57.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:57.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What value was used for the weight decay?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:57.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m2021. Jurassic-1: Technical details and\n",
            "evaluation. White Paper. AI21 Labs , 1.\n",
            "Stephanie Lin, Jacob\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:59.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m0:1\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:59.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many benchmarks were tested?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:33:59.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mExact match performance.\n",
            "3.1 Common Sense Reasoning\n",
            "We consider eight standard common sense rea-\n",
            "son\u001b[0m\n",
            "\u001b[32m2025-02-04 15:34:00.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m20\u001b[0m\n",
            "\u001b[32m2025-02-04 15:34:00.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Was the model compared against GPT-4?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:34:00.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWe follow\n",
            "the QA prompt style used in Ouyang et al. (2022), and\n",
            "report the performance of GPT-3 from\u001b[0m\n",
            "\u001b[32m2025-02-04 15:34:01.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:34:01.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:01.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can LLMs re-produce biases that exist in training data?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:01.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m(2022)). The details of the\n",
            "performance on MMLU on the 57 tasks can be\n",
            "found in Table 16 of the appe\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:03.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:03.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Do authors consider the evaluations enough to fully comprehend the risks of the model?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:03.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m(2022)). The details of the\n",
            "performance on MMLU on the 57 tasks can be\n",
            "found in Table 16 of the appe\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:04.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:04.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does LLaMA compare worse than GPT-3 on average for the CrowS-Paris bias test?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:04.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWe compare the level of bi-\n",
            "ases contained in LLaMA-65B with OPT-175B and\n",
            "GPT3-175B. Higher score in\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:05.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:05.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://assets.publishing.service.gov.uk/media/65c3b5d628a4a00012d2ba5c/6.8558_CO_Generative_AI_Framework_Report_v7_WEB.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:07.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://assets.publishing.service.gov.uk/media/65c3b5d628a4a00012d2ba5c/6.8558_CO_Generative_AI_Framework_Report_v7_WEB.pdf to 6.8558_CO_Generative_AI_Framework_Report_v7_WEB.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:07.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 168 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 6/6 [00:00<00:00,  9.11it/s]\n",
            "\u001b[32m2025-02-04 15:35:10.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:10.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which of the following is not considered a limitation of the Large Language Models? -A: Hallucination -B: Explainability -C: Memorization\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:10.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIt can help you \n",
            "to compare the capabilities of a large number of language models. Here are some of \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([168, 508, 128])\n",
            "doc_masks: torch.Size([168, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:35:11.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mI need more info\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:11.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can LLMs be used as an alternative to visiting a doctor?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:11.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLLMs, in particular, \n",
            "rely heavily on computational power both during their training phase and then \u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:12.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:12.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which of the following is NOT mentioned as a relevant legal or regulatory provision regarding the use of AI technologies? -A: UK data protection law -B: The Online Safety Act -C: Digital, Data and Technology (DDaT) Playbook?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:13.003\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mAny public sector buyers interested in shaping CCS’s longer term commercial agreement \n",
            "portfolio sho\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:14.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:14.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: what are the three key elements to consider when establishing accountable practices across the AI lifecycle? -A: Innovation, efficiency, and cost-effectiveness. -B: Answerability, auditability, and liability. -C: Speed, scalability, and security.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:14.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mUse external resources and emerging best practice, such as data cards and \n",
            "model cards for internal \u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:15.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:15.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What does the text suggest end-users should do when using generative AI for tasks like drafting emails and reports? -A: Assume that the output is inherently accurate and truthful without additional checks. -B:  Assume responsibility for the output and check that it is factually correct, non-discriminatory, and does not violate existing guidelines. -C: Rely on the provider's terms of use for all compliance and accuracy checks.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:15.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIt then selects and presents \n",
            "the response that has the highest probability of being the right fit f\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:16.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:16.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:16.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf to Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:16.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 143 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 5/5 [00:00<00:00,  8.54it/s]\n",
            "\u001b[32m2025-02-04 15:35:19.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:19.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: According to the guide, what is the typical license used to grant reuse rights with libre open access? -A: GNU General Public License -B: Creative Commons license -C: MIT license\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:19.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne of these options is open access.\n",
            "The basic idea of open access is that it makes \n",
            "copyrightable w\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([143, 508, 128])\n",
            "doc_masks: torch.Size([143, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:35:20.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:35:20.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:20.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: how many peer-reviewed open access journals are indexed by the Directory of Open Access Journals (DOAJ)? -A: Over 10,000 -B: Over 20,000 -C: Exactly 30,000\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:20.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor authors of articles, a good \n",
            "place to start is the Directory of Open Access Journals (“DOAJ”), a\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:21.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:21.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does open access eliminate price barriers?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:21.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne of these options is open access.\n",
            "The basic idea of open access is that it makes \n",
            "copyrightable w\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:23.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:23.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Are publication fees required for all open access journals?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:23.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1morg/how-we-work/general-information/open-access-policy .\n",
            "4. For example, the Budapest Open Access In\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:25.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:25.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In what year did the Bill and Melinda Gates foundation implement an open access policy?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:25.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m22 \n",
            "Under the policy, many federal agencies are required to develop plans to make the published resu\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:26.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m2015\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:26.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Are Gold Open Access and Green Open Access mutually exclusive.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:26.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIf you opt to use the Green Open Access  \n",
            "model, you will then need to select the best online \n",
            "venue\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:27.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:27.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://commission.europa.eu/document/download/1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:28.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://commission.europa.eu/document/download/1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf to 1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:28.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 364 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.77it/s]\n",
            "\u001b[32m2025-02-04 15:36:35.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:35.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which type of water must be supplied in a toilet sink? -A: hot -B: cold -C: hot and cold\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:35.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWelfare facilities for areas other than offices (restaurants, k itchens, conference rooms, daycare \n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([364, 508, 128])\n",
            "doc_masks: torch.Size([364, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:36:36.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:36.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In which type of parkings must a carbon monoxide detector be installed? -A: indoor -B: underground -C: indoor or underground\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:36.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mI.2.8.    GAS DETECTION AND VE NTING  \n",
            "The requirements outlined below must be met in addition to th\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:37.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:37.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What percentage is the daylight factor required for façades with exterior obstructions? -A: 0.7% -B: 80% -C: 0.77%\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:37.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m4.    VISUAL COMFORT  \n",
            "4.1.    Natural lighting  \n",
            "Natural light is required for all permanent wor k \u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:38.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:38.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What fire resistance must vertical partitions have? -A: EI30 -B: EI60 -C: EI90\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:38.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe fire -resistant properties of the coverings for the vertical partitions, ceilings, flooring or \n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:39.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:39.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:40.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf to CUDA_C_Programming_Guide.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:36:40.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 1803 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 57/57 [00:10<00:00,  5.55it/s]\n",
            "\u001b[32m2025-02-04 15:37:08.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:37:08.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([1803, 508, 128])\n",
            "doc_masks: torch.Size([1803, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:38:08.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum number of threads within a thread block?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:08.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOn current GPUs, a thread block may contain up to 1024 threads.\n",
            "However, a kernel can be executed by\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:10.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m1024\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:10.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you identify a thread with a four-dimensional index?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:10.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis provides a natural way\n",
            "to invoke computation across the elements in a domain such as a vector, \u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:11.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:11.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In the offline compilation process using nvcc, what happens to the device code? -A: It is directly executed on the host CPU. -B:  It is transformed into assembly and/or binary form. -C:  It is ignored and not used in the final application.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:11.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis section gives an overview of nvcc workflow and command\n",
            "options. A complete description can be f\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:12.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:12.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What are the two ways the host code can be output after being processed by nvcc? -A: As executable machine code, or a interpreted scripting language file. -B: As C++ code for later compilation, or as object code directly. -C: As an encrypted file or in a platform specific assembly format.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:12.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe modified host code is output either as C++ code that is left to be compiled using another tool o\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:14.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:14.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the primary purpose of just-in-time (JIT) compilation? -A: To convert host code into device code for execution on the GPU. -B: To optimize the performance of host code before it is compiled. -C: To compile PTX code into binary code at runtime by the device driver.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:14.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis environment vari-\n",
            "able can be used to validate\n",
            "that PTX code is embedded in\n",
            "an application and \u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:15.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:15.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What happens to the compiled binary code after JIT compilation by the device driver? -A: It is cached for later use and to avoid recompilation. -B: It's directly interpreted and doesn't need to be cached. -C: It is deleted after use to save space.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:15.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe modified host code is output either as C++ code that is left to be compiled using another tool o\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:16.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:16.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: When are virtual addresses assigned to graph allocations? -A: At the moment the graph is executed in the GPU. -B: When the allocation is actually being used by the execution. -C: When the node is created.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:16.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile these virtual addresses\n",
            "are fixed for the lifetime of the allocation node, the allocation cont\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:17.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:17.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What do graph memory nodes represent in a CUDA graph? -A: Actions like allocating or freeing the memory. -B: Code that executes on the CPU to allocate memory. -C: The control flow and branching within a graph.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:18.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mGraph memory nodes are\n",
            "only supported on driver versions 11.4 and newer.\n",
            "379\n",
            "CUDA C++ Programming Gu\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:19.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:19.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: When does a graph allocation's lifetime end? -A: Only when the application shuts down. -B: When the execution reaches the freeing graph node, `cudaFreeAsync()`, or `cudaFree()`. -C: Only when the graph is destroyed.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:19.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile these virtual addresses\n",
            "are fixed for the lifetime of the allocation node, the allocation cont\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:20.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mB\u001b[0m\n",
            "\u001b[32m2025-02-04 15:38:20.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:20.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How must operations accessing graph memory be ordered within a graph? -A: Before the allocation node and after the freeing node. -B: After any previous GPU execution. -C: After the allocation node and before the freeing operation.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:20.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA program must guarantee that operations accessing graph memory:\n",
            "▶are ordered after the allocation n\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:22.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:22.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the primary benefit of Lazy Loading? -A: It reduces memory overhead and saves initialization time. -B: It allows programs to load all kernels faster during initialization. -C: It makes CUDA programs easier to debug and optimize.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:22.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLazy Loading\n",
            "23.1. What is Lazy Loading?\n",
            "Lazy Loading delays loading of CUDA modules and kernels fro\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:23.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:23.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you enable lazy loading by setting the env var `CUDA_MODULE_DATA_LOADING`?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:23.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLazy Loading\n",
            "23.1. What is Lazy Loading?\n",
            "Lazy Loading delays loading of CUDA modules and kernels fro\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:24.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mI need more info\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:24.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:25.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf to 7DUME_EN01_Rules.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:25.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 17 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 1/1 [00:00<00:00, 13.42it/s]\n",
            "\u001b[32m2025-02-04 15:39:27.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:27.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many chapters does the game last?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:27.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m1\n",
            "Will you play as the Fellowship of the Ring to defend the free races and destroy the One Ring?  \n",
            "O\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([17, 508, 128])\n",
            "doc_masks: torch.Size([17, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:39:29.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:29.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory conditions are there?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:29.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:30.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:30.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many different races are there?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:30.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m• \n",
            " Re\n",
            "veal tokens to both players. There is no hidden information.Bonus spaces\n",
            "2 matching Race symb\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:31.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mI need more info\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:31.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which player begins the game? -A: Sauron -B: The Fellowship -C: Other\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:31.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mImmediately place a Fortress pawn on the corresponding region of the central board and benefit  from\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:32.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:32.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you take a Chapter card and a Landmark tile on your same turn?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:32.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:33.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:33.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many goins does a player take when discarding a card during Chapter 3?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:33.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThen, play it in front of you or discard it.\n",
            "  Preparing a chapter\n",
            "At the start of each chapter (1, \u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:34.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:39:34.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:34.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: After taking a landmark tile, do you reveal a new tile and the end of your turn?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:34.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:36.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:36.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can a player pay coins to compensate for missing skill symbols in a Landmark Tile?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:36.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn addition to its effect  (see page 5), it has a chaining  symbol \n",
            "2 .In chapter 2, you may play th\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:37.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:37.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If a player is missing 2 skill symbols, how many coins must they pay to the reserve?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:37.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, the additional cost of your first tile is 0 Coins.\n",
            "  Skills\n",
            "In order to play them, tiles \u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:38.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m2\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:38.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you use a symbol more than once per turn?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:38.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:40.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:40.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which type of cards provide coins? -A: Gray -B: Yellow -C: Blue\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:40.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, the additional cost of your first tile is 0 Coins.\n",
            "  Skills\n",
            "In order to play them, tiles \u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:41.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mI need more info\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:41.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: During which chapter the purple cards become available? -A: Chapter 1 -B: Chapter 2 -C: Chapter 3\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:42.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:43.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mC\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:43.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If you place or move an unit and an enemy fortress is present, does it trigger a conflict?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:43.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIf one or mor e enemy Units are present: trigger a conflict. Each player removes one of their Units \u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:44.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:44.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:44.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:46.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:46.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In how many regions do you need to be present to win the game?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:46.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:47.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m7\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:47.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/is_eotn_rulebook.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:48.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/is_eotn_rulebook.pdf to is_eotn_rulebook.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:48.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 48 documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00,  9.87it/s]\n",
            "\u001b[32m2025-02-04 15:40:50.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-02-04 15:40:50.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([48, 508, 128])\n",
            "doc_masks: torch.Size([48, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-02-04 15:41:50.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum number of cards a player may acquire during the lookout phase?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:50.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m13. At the beginning of the game, each player draws 5 cards from their \n",
            "Clan deck, looks through the\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:51.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m4\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:51.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is there a limit to the number of cards a player may have in their hand?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:51.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNOTE 1:  There’s no limit to the number of cards a player may have \n",
            "in their hand.  \n",
            "NOTE 2:  If the\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:52.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:52.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you raid the locations of a player that has passed during the action phase?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:52.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mStarting with the First player and continuing clockwise, \n",
            "each player performs one action at a time.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:53.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:53.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many points in the scoreboard must be reached during the Action phase to trigger the final round?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:53.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>Discard any remaining, face-up Island cards and reveal new ones.\n",
            " >Pass the First player marker to \u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:55.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m25\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:55.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can players conquer and pillage the same island during the expedition phase?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:55.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>There is no limit to the number, type, or order of \n",
            "actions a player may take during the Action pha\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:57.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:57.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Do you need a fish to conquer a distant island?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:57.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mRations are needed for the long journey!\n",
            "A player can choose to Pillage  a selected Island card with\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:58.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:58.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory points you get from each conquered island?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:41:58.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mEach \n",
            "action draws the clans closer to becoming the greatest empire! The \n",
            "game ends in the same roun\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:00.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m1\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:00.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is there a cleanup phase in the final round?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:00.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mGAME FLOW\n",
            "Note for Imperial Settlers fans \n",
            "You cannot Spend 2 Workers  \n",
            "to get a Resource or a card.\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:01.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:01.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory points are granted by a built Field Location card that work as an upgrade?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:01.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIMPORT ANT: Some Field Locations work only as upgrades. These Fields have \n",
            "the Resources on the righ\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:02.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m1\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:42:02.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:02.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Do you need a ship to be on the expedition board to use a card that allows to pillage or conquer right away?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:02.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNOTE 2: Some abilities in the \n",
            "game have a ‘/’ divider between \n",
            "presented choices. This should be \n",
            "t\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:04.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:04.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you use the raid action without a Raze token?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:04.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThus allowing a player to play \n",
            "a single Boost card or build a single Field Location before resolvin\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:05.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mNO\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:05.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:05.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
            "(Gold tokens assigned to cards are\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:07.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mYES\n",
            "\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:07.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If player 1 has 30 Victory points and 4 workers and player 2 has 30 Victory points and 3 workers, who wins the game? -A: Player 1 -B: Player 2 -C: It's a tie\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:07.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
            "(Gold tokens assigned to cards are\u001b[0m\n",
            "\u001b[32m2025-02-04 15:43:08.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mA\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logger.info(\"Loading input data\")\n",
        "data = pd.read_csv(\"structured_qa.csv\")\n",
        "data[\"pred_answer\"] = [None] * len(data)\n",
        "data[\"pred_section\"] = [None] * len(data)\n",
        "for document_link, document_data in data.groupby(\"document\"):\n",
        "    logger.info(f\"Downloading document {document_link}\")\n",
        "    downloaded_document = Path(f\"{Path(document_link).name}.pdf\")\n",
        "    if not Path(downloaded_document).exists():\n",
        "        urlretrieve(document_link, downloaded_document)\n",
        "        logger.info(f\"Downloaded {document_link} to {downloaded_document}\")\n",
        "    else:\n",
        "        logger.info(f\"File {downloaded_document} already exists\")\n",
        "\n",
        "    answers, sections = process_document(downloaded_document, document_data, model)\n",
        "\n",
        "    for index in document_data.index:\n",
        "        data.loc[index, \"pred_answer\"] = str(answers[index]).upper()\n",
        "        data.loc[index, \"pred_section\"] = sections[index]\n",
        "\n",
        "data.to_csv(\"results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "mltqL7Bhq3m1",
        "outputId": "2ff97f53-d4f1-45a4-856e-2d1fff12a819"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                                           document  \\\n",
              "10          10                   https://arxiv.org/pdf/1706.03762   \n",
              "26          26  https://authorsalliance.org/wp-content/uploads...   \n",
              "28          28                   https://arxiv.org/pdf/2201.11903   \n",
              "29          29                   https://arxiv.org/pdf/2201.11903   \n",
              "33          33                   https://arxiv.org/pdf/2201.11903   \n",
              "34          34                   https://arxiv.org/pdf/2201.11903   \n",
              "37          37  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "42          42  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "45          45  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "57          57  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "73          73  https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...   \n",
              "78          78  https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...   \n",
              "92          92                   https://arxiv.org/pdf/2302.13971   \n",
              "98          98  https://assets.publishing.service.gov.uk/media...   \n",
              "\n",
              "                       type  \\\n",
              "10         Scientific Paper   \n",
              "26  Techincal Documentation   \n",
              "28        Scientific Report   \n",
              "29        Scientific Report   \n",
              "33        Scientific Report   \n",
              "34        Scientific Report   \n",
              "37               Board Game   \n",
              "42               Board Game   \n",
              "45               Board Game   \n",
              "57               Board Game   \n",
              "73  Techincal Documentation   \n",
              "78  Techincal Documentation   \n",
              "92        Scientific Report   \n",
              "98               Regulation   \n",
              "\n",
              "                                              section  \\\n",
              "10                                 5.4 Regularization   \n",
              "26  CHAPTER 5: WHERE DO YOU WANT TO MAKE YOUR WORK...   \n",
              "28                             3.1 Experimental Setup   \n",
              "29                             3.1 Experimental Setup   \n",
              "33                 3.4 Robustness of Chain of Thought   \n",
              "34                                        3.2 Results   \n",
              "37                              CARD AND TILE EFFECTS   \n",
              "42                                CARD AND TILE COSTS   \n",
              "45                              CARD AND TILE EFFECTS   \n",
              "57                                      CLEANUP PHASE   \n",
              "73                             15.3. API Fundamentals   \n",
              "78                        23.1. What is Lazy Loading?   \n",
              "92                                      2.3 Optimizer   \n",
              "98              Limitations of generative AI and LLMs   \n",
              "\n",
              "                                             question answer  \\\n",
              "10  What was the dropout rate used for the base mo...    0.1   \n",
              "26  Are Gold Open Access and Green Open Access mut...     NO   \n",
              "28     How many large language models were evaluated?      5   \n",
              "29  How many benchmarks were used to evaluate arit...      5   \n",
              "33  How many annotators provided independent chain...      3   \n",
              "34  How many random samples were examined to under...    100   \n",
              "37                How many different races are there?      6   \n",
              "42  Can a player pay coins to compensate for missi...    YES   \n",
              "45  Which type of cards provide coins? -A: Gray -B...      B   \n",
              "57       Is there a cleanup phase in the final round?     NO   \n",
              "73  When are virtual addresses assigned to graph a...      C   \n",
              "78  Can you enable lazy loading by setting the env...     NO   \n",
              "92          What value was used for the weight decay?    0.1   \n",
              "98  Which of the following is not considered a lim...      C   \n",
              "\n",
              "         pred_answer  pred_section  \n",
              "10             0.  1           NaN  \n",
              "26               YES           NaN  \n",
              "28              FIVE           NaN  \n",
              "29              FIVE           NaN  \n",
              "33             THREE           NaN  \n",
              "34                50           NaN  \n",
              "37  I NEED MORE INFO           NaN  \n",
              "42                NO           NaN  \n",
              "45  I NEED MORE INFO           NaN  \n",
              "57               YES           NaN  \n",
              "73                 A           NaN  \n",
              "78  I NEED MORE INFO           NaN  \n",
              "92               0:1           NaN  \n",
              "98  I NEED MORE INFO           NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5d3555a-c8da-434c-b5f5-f3546ef586de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>document</th>\n",
              "      <th>type</th>\n",
              "      <th>section</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>pred_answer</th>\n",
              "      <th>pred_section</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>https://arxiv.org/pdf/1706.03762</td>\n",
              "      <td>Scientific Paper</td>\n",
              "      <td>5.4 Regularization</td>\n",
              "      <td>What was the dropout rate used for the base mo...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.  1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>https://authorsalliance.org/wp-content/uploads...</td>\n",
              "      <td>Techincal Documentation</td>\n",
              "      <td>CHAPTER 5: WHERE DO YOU WANT TO MAKE YOUR WORK...</td>\n",
              "      <td>Are Gold Open Access and Green Open Access mut...</td>\n",
              "      <td>NO</td>\n",
              "      <td>YES</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
              "      <td>Scientific Report</td>\n",
              "      <td>3.1 Experimental Setup</td>\n",
              "      <td>How many large language models were evaluated?</td>\n",
              "      <td>5</td>\n",
              "      <td>FIVE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
              "      <td>Scientific Report</td>\n",
              "      <td>3.1 Experimental Setup</td>\n",
              "      <td>How many benchmarks were used to evaluate arit...</td>\n",
              "      <td>5</td>\n",
              "      <td>FIVE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
              "      <td>Scientific Report</td>\n",
              "      <td>3.4 Robustness of Chain of Thought</td>\n",
              "      <td>How many annotators provided independent chain...</td>\n",
              "      <td>3</td>\n",
              "      <td>THREE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
              "      <td>Scientific Report</td>\n",
              "      <td>3.2 Results</td>\n",
              "      <td>How many random samples were examined to under...</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>Board Game</td>\n",
              "      <td>CARD AND TILE EFFECTS</td>\n",
              "      <td>How many different races are there?</td>\n",
              "      <td>6</td>\n",
              "      <td>I NEED MORE INFO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>Board Game</td>\n",
              "      <td>CARD AND TILE COSTS</td>\n",
              "      <td>Can a player pay coins to compensate for missi...</td>\n",
              "      <td>YES</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>45</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>Board Game</td>\n",
              "      <td>CARD AND TILE EFFECTS</td>\n",
              "      <td>Which type of cards provide coins? -A: Gray -B...</td>\n",
              "      <td>B</td>\n",
              "      <td>I NEED MORE INFO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>Board Game</td>\n",
              "      <td>CLEANUP PHASE</td>\n",
              "      <td>Is there a cleanup phase in the final round?</td>\n",
              "      <td>NO</td>\n",
              "      <td>YES</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>73</td>\n",
              "      <td>https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...</td>\n",
              "      <td>Techincal Documentation</td>\n",
              "      <td>15.3. API Fundamentals</td>\n",
              "      <td>When are virtual addresses assigned to graph a...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>78</td>\n",
              "      <td>https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...</td>\n",
              "      <td>Techincal Documentation</td>\n",
              "      <td>23.1. What is Lazy Loading?</td>\n",
              "      <td>Can you enable lazy loading by setting the env...</td>\n",
              "      <td>NO</td>\n",
              "      <td>I NEED MORE INFO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>92</td>\n",
              "      <td>https://arxiv.org/pdf/2302.13971</td>\n",
              "      <td>Scientific Report</td>\n",
              "      <td>2.3 Optimizer</td>\n",
              "      <td>What value was used for the weight decay?</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0:1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>https://assets.publishing.service.gov.uk/media...</td>\n",
              "      <td>Regulation</td>\n",
              "      <td>Limitations of generative AI and LLMs</td>\n",
              "      <td>Which of the following is not considered a lim...</td>\n",
              "      <td>C</td>\n",
              "      <td>I NEED MORE INFO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5d3555a-c8da-434c-b5f5-f3546ef586de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5d3555a-c8da-434c-b5f5-f3546ef586de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5d3555a-c8da-434c-b5f5-f3546ef586de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b8c9a10-bba3-4c25-9aec-a8795990c22e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b8c9a10-bba3-4c25-9aec-a8795990c22e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b8c9a10-bba3-4c25-9aec-a8795990c22e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26,\n        \"min\": 10,\n        \"max\": 98,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          57,\n          78,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf\",\n          \"https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf\",\n          \"https://arxiv.org/pdf/1706.03762\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Techincal Documentation\",\n          \"Regulation\",\n          \"Scientific Report\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"2.3 Optimizer\",\n          \"23.1. What is Lazy Loading?\",\n          \"5.4 Regularization\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Is there a cleanup phase in the final round?\",\n          \"Can you enable lazy loading by setting the env var `CUDA_MODULE_DATA_LOADING`?\",\n          \"What was the dropout rate used for the base model?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"B\",\n          \"NO\",\n          \"6\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"A\",\n          \"YES\",\n          \"I NEED MORE INFO\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_section\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "results = pd.read_csv(\"results.csv\")\n",
        "for index, result in results.iterrows():\n",
        "    results.loc[index, \"pred_answer\"] = result[\"pred_answer\"].strip()\n",
        "    if result[\"pred_answer\"].startswith(\n",
        "        (f\"-{result['answer']}\", f\"{result['answer']}\")\n",
        "    ):\n",
        "        results.loc[index, \"pred_answer\"] = result[\"answer\"]\n",
        "results.loc[results[\"answer\"] != results[\"pred_answer\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4z9XxXWq3m1",
        "outputId": "9a3b50f4-d793-4e72-f897-a78dfc7e7d55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8640776699029126"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "accuracy = sum(results[\"answer\"] == results[\"pred_answer\"]) / len(results)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UXg_TC7R28QI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fcbeac06bbec4921b0c45d61e1e89b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_153cbeb65e6f401aac9648d8b11046ff",
              "IPY_MODEL_f0972bb4d4634bab9cda10b2baa31a71",
              "IPY_MODEL_0803dde3898641b2a8f79ab88eb0653f"
            ],
            "layout": "IPY_MODEL_e26c645517e54af1ade0e052394e9628"
          }
        },
        "153cbeb65e6f401aac9648d8b11046ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06f8db65bac463c98ecb34f2dcb0dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_3415497210a047d78a24086b9f88f91c",
            "value": "artifact.metadata: 100%"
          }
        },
        "f0972bb4d4634bab9cda10b2baa31a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee7f2b57a99c4189b1429db4e6bce3fb",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98ff4a5a268a458da6a7206ed3e437d7",
            "value": 1633
          }
        },
        "0803dde3898641b2a8f79ab88eb0653f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70a80cb1055d4786aa1f8a84ef3157ce",
            "placeholder": "​",
            "style": "IPY_MODEL_794d0b6347ab45b6ac7bf2d7eeb630e4",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 48.3kB/s]"
          }
        },
        "e26c645517e54af1ade0e052394e9628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f06f8db65bac463c98ecb34f2dcb0dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3415497210a047d78a24086b9f88f91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee7f2b57a99c4189b1429db4e6bce3fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ff4a5a268a458da6a7206ed3e437d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70a80cb1055d4786aa1f8a84ef3157ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794d0b6347ab45b6ac7bf2d7eeb630e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b38d9d92f09428a9d9670f918599e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e14603a547e8473fa824cc7685b62456",
              "IPY_MODEL_fab6bb0faa174a4aa02366b037660fa0",
              "IPY_MODEL_b4832b95793c4d308d4d662c031f0b51"
            ],
            "layout": "IPY_MODEL_5bab6e216cdd4fd7a1d514d91056f275"
          }
        },
        "e14603a547e8473fa824cc7685b62456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e5af6381ee424e92e17622294ce455",
            "placeholder": "​",
            "style": "IPY_MODEL_b38ffca43a2942c78216e663d3712dc2",
            "value": "config.json: 100%"
          }
        },
        "fab6bb0faa174a4aa02366b037660fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b232acd3b4f240e8b9815454dd78b64d",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b4c805c2daf4e7dafbd6aedc458b4ef",
            "value": 743
          }
        },
        "b4832b95793c4d308d4d662c031f0b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cb7af363f1a4372aec5826e2dc4d7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1b59bbf7535945c2b290f4068e02214d",
            "value": " 743/743 [00:00&lt;00:00, 40.5kB/s]"
          }
        },
        "5bab6e216cdd4fd7a1d514d91056f275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e5af6381ee424e92e17622294ce455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38ffca43a2942c78216e663d3712dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b232acd3b4f240e8b9815454dd78b64d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4c805c2daf4e7dafbd6aedc458b4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cb7af363f1a4372aec5826e2dc4d7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b59bbf7535945c2b290f4068e02214d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5031f4a757d43ec87fc4dd068773aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11e03bd58cf04306aaaa1ddaf9b30c38",
              "IPY_MODEL_006194e2d15246c8ac0006f4ad0ec3d5",
              "IPY_MODEL_6c0db984c21f4b668af8895961c74a91"
            ],
            "layout": "IPY_MODEL_b02f570cc69f4da78aad558e9d1c3502"
          }
        },
        "11e03bd58cf04306aaaa1ddaf9b30c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f8c4648e71d4297ab315f8f84814d16",
            "placeholder": "​",
            "style": "IPY_MODEL_fd9a1d362a2c4aa4a9217b7913aab50b",
            "value": "model.safetensors: 100%"
          }
        },
        "006194e2d15246c8ac0006f4ad0ec3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52896569fd8c47a58f7f1e04dc2982ad",
            "max": 438349816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d60dbc988c547168f59fded596f6326",
            "value": 438349816
          }
        },
        "6c0db984c21f4b668af8895961c74a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c6634d9446477caddf5bc0baf79b74",
            "placeholder": "​",
            "style": "IPY_MODEL_481d046b1c2c427abea771e6a900be4b",
            "value": " 438M/438M [00:02&lt;00:00, 236MB/s]"
          }
        },
        "b02f570cc69f4da78aad558e9d1c3502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8c4648e71d4297ab315f8f84814d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9a1d362a2c4aa4a9217b7913aab50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52896569fd8c47a58f7f1e04dc2982ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d60dbc988c547168f59fded596f6326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91c6634d9446477caddf5bc0baf79b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "481d046b1c2c427abea771e6a900be4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46246ea8cd134dcb952f2009bed30ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c55bbfdc199749a2b2c4e6514b96af71",
              "IPY_MODEL_228881dd188447f89f6d5a5e30ba2b39",
              "IPY_MODEL_e8fd851273e14870824433d33f71d521"
            ],
            "layout": "IPY_MODEL_69428342766b4ec28848d8cce5c88328"
          }
        },
        "c55bbfdc199749a2b2c4e6514b96af71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b280d7fd36e243f49a55ea5c90b3d96b",
            "placeholder": "​",
            "style": "IPY_MODEL_8d226412b4e749839f8ed0efc23f3456",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "228881dd188447f89f6d5a5e30ba2b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_858afce4603c4ac4ad6ae8a9817adfc6",
            "max": 405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcf8e5b3abed4bd49df30e3235bbbc5d",
            "value": 405
          }
        },
        "e8fd851273e14870824433d33f71d521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b789dd69092449478667563eb96ae51b",
            "placeholder": "​",
            "style": "IPY_MODEL_2df45b5f52884ad689c0f7da99f9046d",
            "value": " 405/405 [00:00&lt;00:00, 19.6kB/s]"
          }
        },
        "69428342766b4ec28848d8cce5c88328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b280d7fd36e243f49a55ea5c90b3d96b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d226412b4e749839f8ed0efc23f3456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "858afce4603c4ac4ad6ae8a9817adfc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf8e5b3abed4bd49df30e3235bbbc5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b789dd69092449478667563eb96ae51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df45b5f52884ad689c0f7da99f9046d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d448a6c57d454df3a40509bebc92d337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff4a3e4282164dbcbd30ad0123ce9966",
              "IPY_MODEL_3570f41c24a04d039f6e92f9c8e57cc3",
              "IPY_MODEL_856f08461e6d4d218306310f38dc00ed"
            ],
            "layout": "IPY_MODEL_1309b4cac445449ca766b881004dd04c"
          }
        },
        "ff4a3e4282164dbcbd30ad0123ce9966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9761af9a6ca14a03a6313fbc4edba9c7",
            "placeholder": "​",
            "style": "IPY_MODEL_de42f1287b0b42f59efe335d8d96cebe",
            "value": "vocab.txt: 100%"
          }
        },
        "3570f41c24a04d039f6e92f9c8e57cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fafe7befa5e46d2890e63327b846172",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d6d834e29140a7b452634cba9223e7",
            "value": 231508
          }
        },
        "856f08461e6d4d218306310f38dc00ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25ed47d1fdf402db19dd3736e79a9cd",
            "placeholder": "​",
            "style": "IPY_MODEL_429b6a5553c24b31acb8d7a19f947d63",
            "value": " 232k/232k [00:00&lt;00:00, 8.54MB/s]"
          }
        },
        "1309b4cac445449ca766b881004dd04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9761af9a6ca14a03a6313fbc4edba9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de42f1287b0b42f59efe335d8d96cebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fafe7befa5e46d2890e63327b846172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d6d834e29140a7b452634cba9223e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d25ed47d1fdf402db19dd3736e79a9cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "429b6a5553c24b31acb8d7a19f947d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42489b00de754d8ebc183a12f649eb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2404b6235374639bbf22cfdc7f2dd84",
              "IPY_MODEL_86b48915fae748eeb3a572e793f8f8c7",
              "IPY_MODEL_fc2dcb10a10244b7b19ca3163c0d5b17"
            ],
            "layout": "IPY_MODEL_03078427cd344cd9bdc5e18da8a6dfa4"
          }
        },
        "f2404b6235374639bbf22cfdc7f2dd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7220498a1cb44256b294c639e016b80e",
            "placeholder": "​",
            "style": "IPY_MODEL_ea5a5b11334243048936cd96d7189348",
            "value": "tokenizer.json: 100%"
          }
        },
        "86b48915fae748eeb3a572e793f8f8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add5098a768f4b8292919aad0f7a95be",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4315a7426a7c4bf7a295ed915cb7af0e",
            "value": 466081
          }
        },
        "fc2dcb10a10244b7b19ca3163c0d5b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1e49d8f8444ced936dee33f632dd83",
            "placeholder": "​",
            "style": "IPY_MODEL_9c2f76016b7b4e819e3cac5bb8afa2b3",
            "value": " 466k/466k [00:00&lt;00:00, 3.24MB/s]"
          }
        },
        "03078427cd344cd9bdc5e18da8a6dfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7220498a1cb44256b294c639e016b80e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5a5b11334243048936cd96d7189348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add5098a768f4b8292919aad0f7a95be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4315a7426a7c4bf7a295ed915cb7af0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b1e49d8f8444ced936dee33f632dd83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2f76016b7b4e819e3cac5bb8afa2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4f77aec2ec947508a6430f99df88cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_050caa495ff1470e9ccbb69e337edf41",
              "IPY_MODEL_a000ee3a60924a73ae9a07605848924c",
              "IPY_MODEL_b1392a03cdb045c2833aaf5370ba9879"
            ],
            "layout": "IPY_MODEL_ef6d766fc5304b34b14ce2722570b785"
          }
        },
        "050caa495ff1470e9ccbb69e337edf41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e141e9becd044fbab6394748b9838a2",
            "placeholder": "​",
            "style": "IPY_MODEL_99a6d80264534dbdb40563b876fad6ba",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "a000ee3a60924a73ae9a07605848924c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5210b33e6f5547bd8689c50506d20949",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2efd2ec934cb485ebb222ba39ef9b5c6",
            "value": 112
          }
        },
        "b1392a03cdb045c2833aaf5370ba9879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e25ac32d79340ff9714bc02aaa5f95a",
            "placeholder": "​",
            "style": "IPY_MODEL_9459c2d1de6d420cb621a139141f429a",
            "value": " 112/112 [00:00&lt;00:00, 6.91kB/s]"
          }
        },
        "ef6d766fc5304b34b14ce2722570b785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e141e9becd044fbab6394748b9838a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a6d80264534dbdb40563b876fad6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5210b33e6f5547bd8689c50506d20949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efd2ec934cb485ebb222ba39ef9b5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e25ac32d79340ff9714bc02aaa5f95a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9459c2d1de6d420cb621a139141f429a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}