{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcx4osZYq3mt"
      },
      "source": [
        "# Structured Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE32hJKeq3mv"
      },
      "source": [
        "Source code: https://github.com/mozilla-ai/structured-qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDIEL7SNq3mv"
      },
      "source": [
        "Docs: https://mozilla-ai.github.io/structured-qa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OwS4mKRq3mv"
      },
      "source": [
        "## GPU Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FYZaTmnq3mw"
      },
      "source": [
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to `Edit`→`Notebook Settings`\n",
        "- Select T4 GPU from the Hardware Accelerator section\n",
        "- Click `Save` and accept.\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RsETkxfq3mw",
        "outputId": "66a5e27f-9b93-402b-cba2-212ac3f83fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPU not available\")\n",
        "else:\n",
        "    print(\"GPU is available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEgVEmSQq3mx"
      },
      "source": [
        "## Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1eAychVq3my",
        "outputId": "ab1ecc14-7c37-46e6-f3ba-a4be47c4dc31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ragatouille\n",
            "  Downloading ragatouille-0.0.8.post4-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting colbert-ai==0.2.19 (from ragatouille)\n",
            "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting fast-pytorch-kmeans==0.2.0.1 (from ragatouille)\n",
            "  Downloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: langchain>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.15)\n",
            "Requirement already satisfied: langchain_core>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.31)\n",
            "Collecting llama-index>=0.7 (from ragatouille)\n",
            "  Downloading llama_index-0.12.14-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting sentence-transformers<3.0.0,>=2.2.2 (from ragatouille)\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting srsly==2.4.8 (from ragatouille)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (4.47.1)\n",
            "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
            "  Downloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Collecting datasets (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (3.1.0)\n",
            "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.67.1)\n",
            "Collecting ujson (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\n",
            "Collecting pynvml (from fast-pytorch-kmeans==0.2.0.1->ragatouille)\n",
            "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (3.11.11)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (4.12.2)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.14 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_core-0.12.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_readers_file-0.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->ragatouille) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.18.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core>=0.1.4->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.23.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.59.9)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille) (1.2.17)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille) (1.6.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_cloud-0.1.11-py3-none-any.whl.metadata (912 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille)\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.7->ragatouille)\n",
            "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (1.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->ragatouille) (3.1.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=1.13->ragatouille)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->ragatouille) (3.0.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.44)\n",
            "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->fast-pytorch-kmeans==0.2.0.1->ragatouille)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain>=0.1.0->ragatouille) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.3.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.14->llama-index>=0.7->ragatouille)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.17.0)\n",
            "Downloading ragatouille-0.0.8.post4-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl (8.8 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.12.14-py3-none-any.whl (6.9 kB)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.3-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.14-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.4-py3-none-any.whl (39 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.11-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: colbert-ai\n",
            "  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114759 sha256=1259c9368306c751f97b0a8a0e9b0b834b62d6afab66dc017b037e175bb4e949\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/75/5f/9680ae93eb0258ccf3e9d8cd34f328c53f8888c06c37067f3a\n",
            "Successfully built colbert-ai\n",
            "Installing collected packages: striprtf, nvidia-ml-py, filetype, dirtyjson, bitarray, xxhash, voyager, ujson, srsly, python-dotenv, PyPDF2, pypdf, pynvml, onnx, ninja, mypy-extensions, marshmallow, fsspec, faiss-cpu, dill, typing-inspect, tiktoken, multiprocess, llama-cloud, git-python, dataclasses-json, llama-index-core, fast-pytorch-kmeans, datasets, sentence-transformers, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, colbert-ai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.5.1\n",
            "    Uninstalling srsly-2.5.1:\n",
            "      Successfully uninstalled srsly-2.5.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.3.1\n",
            "    Uninstalling sentence-transformers-3.3.1:\n",
            "      Successfully uninstalled sentence-transformers-3.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 bitarray-3.0.0 colbert-ai-0.2.19 dataclasses-json-0.6.7 datasets-3.2.0 dill-0.3.8 dirtyjson-1.0.8 faiss-cpu-1.9.0.post1 fast-pytorch-kmeans-0.2.0.1 filetype-1.2.0 fsspec-2024.9.0 git-python-1.0.3 llama-cloud-0.1.11 llama-index-0.12.14 llama-index-agent-openai-0.4.3 llama-index-cli-0.4.0 llama-index-core-0.12.14 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.14 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.4 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.20 marshmallow-3.26.0 multiprocess-0.70.16 mypy-extensions-1.0.0 ninja-1.11.1.3 nvidia-ml-py-12.570.86 onnx-1.17.0 pynvml-12.0.0 pypdf-5.2.0 python-dotenv-1.0.1 ragatouille-0.0.8.post4 sentence-transformers-2.7.0 srsly-2.4.8 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0 ujson-5.10.0 voyager-2.1.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "%pip install ragatouille PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0dl5xGnq3my",
        "outputId": "395263da-3e7c-469b-ed18-f4e34a8a3774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark\n",
            "  Cloning https://github.com/mozilla-ai/structured-qa.git (to revision 5-add-benchmark) to /tmp/pip-req-build-fztvdq23\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mozilla-ai/structured-qa.git /tmp/pip-req-build-fztvdq23\n",
            "  Running command git checkout -b 5-add-benchmark --track origin/5-add-benchmark\n",
            "  Switched to a new branch '5-add-benchmark'\n",
            "  Branch '5-add-benchmark' set up to track remote branch '5-add-benchmark' from 'origin'.\n",
            "  Resolved https://github.com/mozilla-ai/structured-qa.git to commit 0b8e5cf9d2db91af71478a715bfdbba1b36316fa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire (from structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev77+g0b8e5cf) (0.27.1)\n",
            "Collecting loguru (from structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev77+g0b8e5cf) (2.10.6)\n",
            "Collecting pymupdf4llm (from structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev77+g0b8e5cf) (6.0.2)\n",
            "Collecting streamlit (from structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->structured-qa==0.3.3.dev77+g0b8e5cf) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev77+g0b8e5cf) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev77+g0b8e5cf) (2.27.2)\n",
            "Collecting pymupdf>=1.24.10 (from pymupdf4llm->structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (1.26.4)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (1.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev77+g0b8e5cf) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev77+g0b8e5cf) (1.17.0)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m508.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: structured-qa, fire\n",
            "  Building wheel for structured-qa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for structured-qa: filename=structured_qa-0.3.3.dev77+g0b8e5cf-py3-none-any.whl size=16202 sha256=471f9739e08b922697b7117495ad211785a46ff89fe92d96a32d34b93fba365b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xj29gks5/wheels/be/a2/66/5bd06ba07afee632d178971d710ae5150fe6379c43e361cd32\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=aaae19261f87cd2a4893f2b83751a95da7e0b5fae55a2d71054ad33c10ceac02\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built structured-qa fire\n",
            "Installing collected packages: watchdog, pymupdf, loguru, fire, pymupdf4llm, pydeck, streamlit, structured-qa\n",
            "Successfully installed fire-0.7.0 loguru-0.7.3 pydeck-0.9.1 pymupdf-1.25.2 pymupdf4llm-0.0.17 streamlit-1.41.1 structured-qa-0.3.3.dev77+g0b8e5cf watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl_haxghq3mz",
        "outputId": "9b2a4855-a3c3-4395-bc32-9a41a9030f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-29 09:01:27--  https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21734 (21K) [text/plain]\n",
            "Saving to: ‘structured_qa.csv’\n",
            "\n",
            "structured_qa.csv   100%[===================>]  21.22K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-01-29 09:01:27 (13.3 MB/s) - ‘structured_qa.csv’ saved [21734/21734]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdWx_e7iq3mz"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vGqX_bU5q3mz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab.userdata import get, SecretNotFoundError\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=get(\"GOOGLE_API_KEY\"))\n",
        "except SecretNotFoundError as e:\n",
        "    raise RuntimeError(\"Please set the GOOGLE_API_KEY secret to your API key\") from e\n",
        "os.environ[\"LOGURU_LEVEL\"] = \"INFO\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cbkIjBYNq3mz"
      },
      "outputs": [],
      "source": [
        "from loguru import logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BiUeBWnIq3mz"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "\n",
        "def load_pdf(pdf_file: str) -> str | None:\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        return \"\\n\".join(page.extract_text() for page in pdf_reader.pages)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0B2yhFISDgG"
      },
      "source": [
        "## Function to Process all questions for a single Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ilxn8LGFq3m0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from ragatouille.data import CorpusProcessor\n",
        "\n",
        "\n",
        "def process_document(\n",
        "    document_file,\n",
        "    document_data,\n",
        "    model,\n",
        "):\n",
        "    logger.info(\"Setting up RAG\")\n",
        "    RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    corpus_processor = CorpusProcessor()\n",
        "    documents = corpus_processor.process_corpus([load_pdf(document_file)])\n",
        "    RAG.encode([x[\"content\"] for x in documents])\n",
        "\n",
        "    logger.info(\"Predicting\")\n",
        "    answers = {}\n",
        "    sections = {}\n",
        "    for index, row in document_data.iterrows():\n",
        "        if model.n > 0 and model.n % 9 == 0:\n",
        "            logger.info(\"Waiting for 60 seconds\")\n",
        "            time.sleep(60)\n",
        "        question = row[\"question\"]\n",
        "        question_part, *options = question.split(\"?\")\n",
        "\n",
        "        logger.info(f\"Question: {question}\")\n",
        "        results = RAG.search_encoded_docs(query=question_part, k=3)\n",
        "        current_info = \"\\n\".join(result[\"content\"] for result in results)\n",
        "        logger.info(current_info[:100])\n",
        "\n",
        "        answer = model.model.generate_content(\n",
        "            [f\"This is the document: {current_info}\", question]\n",
        "        )\n",
        "        logger.info(answer.text)\n",
        "        answers[index] = json.loads(answer.text)[\"answer\"]\n",
        "        sections[index] = None\n",
        "        model.n += 1\n",
        "\n",
        "    return answers, sections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr3ke2aJq3m0"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zKMHc0Ouq3m0"
      },
      "outputs": [],
      "source": [
        "from structured_qa.model_loaders import load_gemini_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cMBl2dxLq3m0"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a rigorous assistant answering questions.\n",
        "You must only answer based on the current information available which is:\n",
        "\n",
        "```\n",
        "{CURRENT_INFO}\n",
        "```\n",
        "\n",
        "If the current information available not enough to answer the question,\n",
        "you must return \"I need more info\" srting and nothing else:\n",
        "\n",
        "If the current information is enough to answer, you must return one of the following formats:\n",
        "- YES/NO (for boolean questions)\n",
        "- Number (for numeric questions)\n",
        "- Single letter (for multiple-choice questions)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QV3pBXvhq3m0"
      },
      "outputs": [],
      "source": [
        "model = load_gemini_model(\"gemini-2.0-flash-exp\", system_prompt=SYSTEM_PROMPT)\n",
        "model.n = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5jWlVBaq3m1"
      },
      "source": [
        "# Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "20d67e8902244d87ad72120b9fb71284",
            "1e7fcaa0156d4f09af4bf24a8607e787",
            "0bad96f6403c4042a9ed7bb491c1b25d",
            "1c9e0ff5abab4e378959f47c5655f9f7",
            "dd0ddf2594eb42b4babe6eeaf6a59bbb",
            "5e532f20ae6d4a5c90d5beba1518d3ee",
            "50215024305b41c38aec0a3808b3bc84",
            "a42220b511f14cd8b89f5071c0d216a4",
            "46097609bd4b46fa94c27a5dcfe98a1a",
            "e3084de2764a48089029ffafa1087e8a",
            "420798f709e2420d81d7223c34ca442e",
            "8209dde69d4147739c522342bfedcccd",
            "066c98c9848e4e00b68d0e98ec6f3c1f",
            "c88515f583bd469ca0d6ca54a812ca14",
            "a47e31ce610b4dcf8ac934ec11aefc65",
            "38bd9b6cec8f42f1a9b2caca71478f4b",
            "c8939bbe84c24ff8ad43c8d996d29af2",
            "9a8317a8c8754d4d8b513a7fb0366c8d",
            "ea0ed18363ec4a86b0383e0b43d38ac7",
            "dfb0d5f3c9ae46dc910d335a9215521a",
            "a2a6f8043e9943c7a6ec9112ac3d33bd",
            "8d18d3f17569471fade4a2df380a245c",
            "29d523b694174b7596944eeb86a553d0",
            "73d7ade0b58b41d1b1ac109026eeabc8",
            "cc1c0fcd84b94a199612c3e7ccd906cd",
            "5dbe5cc9d6e64e5cb62e7018a42e1f8e",
            "56de5716ee0146158e399759aef55c41",
            "7f05356467fa4c2ab321004efa06e9c9",
            "ed08c56e20194dbca6732642fb4af466",
            "266e8497e8b04e3fad5d23391960ed13",
            "0bebf69871bb4d04a5329ecb32d64b06",
            "be5d5dcca8cb498d8eb982b1cc1273fc",
            "f6e97fcb881443beaec839bd64530d2d",
            "d1b166882cef441c816a75b784b3dcb0",
            "5f145f7ffcd540149cd775f01e3da418",
            "6766b3d159fd4c29b853f3ad44616429",
            "e3ec24ca9f384b6e8a6b25f66c9a2872",
            "5b034562b2354e70a27bc06f5fe674cd",
            "f76cfc2d293d4b409e4fc8bfa805af96",
            "167d14dc1f3b42fe9f4d9cc2ec341363",
            "3919381f1ae247219c7e4378a5d2e1ff",
            "2a59d91e7621422ebda4fefca0ee6760",
            "d44706bfc8494edc8f266d3a94ff16a2",
            "13dd434100e747588f8be140f55305a3",
            "81a8270d87ef4c9b80c46c5236c8292f",
            "7d4aa0529fb74e81a08cc12aeb243456",
            "67f985db0d7b41f7b15f135d6acb039e",
            "6da62a5ad31940329f00748ad6eab4da",
            "27ec9d176d11451bb049b62c278a86ff",
            "4e15263fae0140299c6a55ce95f7bd43",
            "cacdb3a3a0e04ca3b744fb82a3dcc925",
            "97d1348ebec44687ac2a9151d52b1e8f",
            "ae77818599cd4bc2ac761865e81c3f15",
            "44b4d7daccdb46f19db7675c3a7d4f49",
            "ee9dca0e0f2c49a49fb50b623818cda9",
            "3bd3d79c0262467296061f64606e57ce",
            "024598891b4f46299dc20b5cfd714e0c",
            "9846ac95a9864f6aad40bffcd1595c48",
            "f7e1a279ca7a4576a67d600c6e0fcad6",
            "1eda4198a078469dbba236c3ed8654c3",
            "45ce30572c20425691ebdabe0696b0ec",
            "667241a7a4e6442b9e32450dbcbb0f56",
            "11c36278698f4a6e8f606811eaff2166",
            "c8a050cfb1164c1cbecb0a86bc555d9c",
            "004ad74940344b6eb376ae4cfc85f26b",
            "cb69dbb882694ed3bab1a2b35e0df524",
            "54af3da7793c404fa8b4e1062185ea68",
            "24ae74e4073749fba785b660dac48f4c",
            "895f37ac364f4c1aa4b3089fa286fca3",
            "f63e1751a94246888bf0426a2288cb36",
            "90076a55ec674636b93c7b1d741ea374",
            "944a78e6adaf4e3a87551d0bd5a6fc75",
            "8d7d0da8d2344625aeef3d1c452a9c68",
            "747558448b5e40038b270a6a6f6af6f0",
            "81ee5fe4f8044ab9819b9f767c41826e",
            "4f1165cdc7ef4701889d0e6de6ac9ed1",
            "1601603b8da04598b2a3b1b6532b9de9"
          ]
        },
        "id": "W9r17Rz3q3m1",
        "outputId": "e4618fa8-bdc7-4d27-938e-2a9fb69643f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:01:52.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mLoading input data\u001b[0m\n",
            "\u001b[32m2025-01-29 09:01:53.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:01:53.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://aiindex.stanford.edu/wp-content/uploads/2024/05/HAI_AI-Index-Report-2024.pdf to HAI_AI-Index-Report-2024.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:01:53.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20d67e8902244d87ad72120b9fb71284",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8209dde69d4147739c522342bfedcccd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d523b694174b7596944eeb86a553d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1b166882cef441c816a75b784b3dcb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81a8270d87ef4c9b80c46c5236c8292f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bd3d79c0262467296061f64606e57ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54af3da7793c404fa8b4e1062185ea68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 1214 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/38 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 38/38 [00:06<00:00,  5.93it/s]\n",
            "\u001b[32m2025-01-29 09:02:49.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:49.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: which type of risk was identified as the leading concern globally? -A: Fairness risks. -B: Privacy and data governance risks. -C: Risks related to generative AI deployment.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:49.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNotably, they observe that these concerns are \n",
            "significantly higher in Asia and Europe compared to \n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([1214, 508, 128])\n",
            "doc_masks: torch.Size([1214, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:02:51.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:51.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In which geographical area were fairness risks selected by the smallest percentage of respondents? -A: Asia. -B: Europe. -C: North America.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:51.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNotably, they observe that these concerns are \n",
            "significantly higher in Asia and Europe compared to \n",
            "\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:52.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:52.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is a major consequence of the rising training costs for foundation models? -A: The exclusion of universities from developing leading-edge foundation models. -B: Increased collaboration between universities and AI companies. -C: A decrease in the number of policy initiatives related to AI research.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:52.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile AI \n",
            "companies seldom reveal the expenses involved \n",
            "in training their models, it is widely beli\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:53.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:53.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How the AI Index and Epoch AI estimated training costs for foundation models? -A: By surveying AI companies on their reported expenses. -B: By analyzing government funding allocated to AI research. -C: By analyzing training duration, hardware type, quantity, and utilization rate.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:53.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mUnderstanding the cost of training AI models is \n",
            "important, yet detailed information on these costs \u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:55.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:55.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is a major source of inequality in AI related to tokenization? -A: The significant variability in the number of tokens required to represent the same content across different languages. -B: The uniform processing speed of all languages. -C: The consistent cost of inference across different languages.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:55.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mthe word “dog”) which can \n",
            "help the model to make a better prediction (“fetch” vs \n",
            "“baseball”). If m\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:57.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:57.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What are the three major inequalities resulting from variable tokenization? -A: Increased model training costs, limited access to resources, and biased results. -B: Higher inference costs, longer processing times, and reduced available context for the model. -C:  Limited language support, increased hardware requirements, and data bias.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:57.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor \n",
            "instance, Portuguese closely matches English in the \n",
            "efficiency of the GPT-4 tokenizer, yet it \u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:58.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:58.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many AI-related regulations were enacted in the United States in 2023?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:02:58.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn 2023, there were 25 AI-related \n",
            "regulations, up from just one in 2016. Last year alone, the total\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:00.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 25\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:00.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which of the following was identified as a high relevance AI regulation? -A: Securities and Exchange Commission’s Cybersecurity Risk Management Strategy. -B: Copyright Office and Library of Congress’ Copyright Registration Guidance. -C: Regulations related to foreign trade and international finance\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:00.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA medium relevance regulation includes meaningful mentions of AI but is not solely centered on it. A\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:02.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:02.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which country had the highest proportion of female bachelor's graduates in informatics, computer science, computer engineering, and information technology among the surveyed European nations? -A: France. -B: Bulgaria. -C: United Kingdom\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:02.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCS, CE, and IT Bachelor’s \n",
            "Graduates\n",
            "In 2022, the United Kingdom led with the highest \n",
            "number of new\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:04.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:03:04.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:04.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which countries reported the smallest proportion of female master's graduates in informatics, CS, CE, and IT as of 2022? -A: Estonia, Romania, and Bulgaria. -B: United Kingdom, Germany, and Switzerland. -C: Belgium, Italy, and Switzerland.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:04.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mDespite some narrowing since 2011, men continue to \n",
            "dominate. For example, France (14.8%), the Unite\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:06.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:06.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/1706.03762\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:06.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/1706.03762 to 1706.03762.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:06.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 56 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00,  8.31it/s]\n",
            "\u001b[32m2025-01-29 09:04:09.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:09.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What type of architecture does the model use? -A: decoder only -B: encoder only -C: encoder-decoder\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:09.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mAt each step the model is auto-regressive\n",
            "[10], consuming the previously generated symbols as additi\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([56, 508, 128])\n",
            "doc_masks: torch.Size([56, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:04:10.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:10.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers compose the encoder?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:10.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mAt each step the model is auto-regressive\n",
            "[10], consuming the previously generated symbols as additi\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:14.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 6\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:14.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers compose the decoder?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:14.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "laye\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:16.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 6\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:16.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many parallel attention heads are used?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:16.205\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
            "dk=dv=dmod\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:17.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 8\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:17.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the final model use learned embeddings for the input and output tokens?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:17.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in o\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:19.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:19.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the final model use learned positional embeddings?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:19.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTo this end, we add \"positional encodings\" to the input embeddings at the\n",
            "bottoms of the encoder and\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:20.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:20.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many GPUs were used for training?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:20.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
            "batch contained a\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:26.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 8\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:26.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What type of GPUs were used for training? -A: NVIDIA A100 -B: NVIDIA P100 -C: NVIDIA T4\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:26.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
            "batch contained a\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:27.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:04:27.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:27.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What optimizer was used for training? -A: AdamW -B: Adam -C: SGD\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:28.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
            "(3.5 days).\n",
            "5.3 Optimizer\n",
            "We used the Adam optimizer [\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:29.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:29.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many warmup steps were used?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:29.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
            "(3.5 days).\n",
            "5.3 Optimizer\n",
            "We used the Adam optimizer [\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:31.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 4000\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:31.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What was the dropout rate used for the base model?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:31.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m36 41.29 7.7·10191.2·1021\n",
            "Transformer (base model) 27.3 38.1 3.3·1018\n",
            "Transformer (big) 28.4 41.8 2.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:32.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": 0.1\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:32.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2106.09685.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:32.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2106.09685.pdf to 2106.09685.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:32.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 137 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 5/5 [00:00<00:00,  9.20it/s]\n",
            "\u001b[32m2025-01-29 09:05:41.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:41.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does LoRA work with any neural network containing dense layers?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:41.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mMore importantly, these method often fail to\n",
            "match the ﬁne-tuning baselines, posing a trade-off betw\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([137, 508, 128])\n",
            "doc_masks: torch.Size([137, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:05:42.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:42.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: By how much can LoRA reduce GPU memory requirements during training? -A: 10x, -B: 5x, -C: 3x\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:42.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mUsing GPT-3 175B as an example – deploying indepen-\n",
            "dent instances of ﬁne-tuned models, each with 17\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:44.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:44.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In billions, how many trainable parameters does GPT-3 have?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:44.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe results\n",
            "on WikiSQL have a ﬂuctuation around \u00060:5%, MNLI-m around \u00060:1%, and SAMSum around\n",
            "\u00060:2/\u0006\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:46.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": 175\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:46.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does LoRA introduce additional inference latency compared to full fine-tuning?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:46.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\n",
            "a sub\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:49.697\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:49.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2201.11903\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:49.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2201.11903 to 2201.11903.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:49.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 199 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 7/7 [00:00<00:00,  7.62it/s]\n",
            "\u001b[32m2025-01-29 09:05:54.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:54.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is Arithmetic reasoning is a task that language models often find very easy?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:54.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
            "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([199, 508, 128])\n",
            "doc_masks: torch.Size([199, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:05:57.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:57.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many large language models were evaluated?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:57.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor AQuA, we used four exemplars\n",
            "and solutions from the training set, as given in Appendix Table 21.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:58.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": 5\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:05:58.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:06:58.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many benchmarks were used to evaluate arithmetic reasoning?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:06:58.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
            "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:00.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 5\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:00.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is symbolic reasoning usually simple for humans but challenging for language models?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:00.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m2We sample examples \u001460tokens to ﬁt into our input context window, and also limit the examples to \u00142\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:04.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:04.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many words have the example names that the model has seen for letter concatenation? -A: 3 -B: 2 -C: 4\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:04.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoebe ﬂips the coin.\n",
            "Osvaldo does not ﬂip the coin. Is the coin still heads up?”\n",
            "!“no” ).\n",
            "As the co\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:06.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:06.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which symbolic reasoning task is used as an out-of-domain evaluation? -A: Coin Flip -B: Tower of Hanoi -C: Chess puzzles\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:07.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoebe ﬂips the coin.\n",
            "Osvaldo does not ﬂip the coin. Is the coin still heads up?”\n",
            "!“no” ).\n",
            "As the co\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:08.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:08.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many annotators provided independent chains of thought?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:08.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSimilar to the\n",
            "annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instruct\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:10.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:10.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many random samples were examined to understand model performance?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:10.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWe also randomly examined 50 random sam-\n",
            "ples for which the model gave the wrong answer.\n",
            "The summary\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:11.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 50\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:11.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2210.05189\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:11.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://arxiv.org/pdf/2210.05189 to 2210.05189.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:11.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 44 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00, 10.13it/s]\n",
            "\u001b[32m2025-01-29 09:07:13.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:13.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can recurrent networks also be converted to decision trees?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:13.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, for continuous activations, the neural\n",
            "network equivalent tree immediately becomes inﬁnit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([44, 508, 128])\n",
            "doc_masks: torch.Size([44, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:07:14.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:14.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers are in the toy model (y = x^2)?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:14.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:16.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:16.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does the toy model (y = x^2) use Sigmoid activation function?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:16.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:17.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:07:17.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:17.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many parameters are in the toy model (y = x^2) tree?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:17.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
            "1WTh(0)+tX\n",
            "i=1ci^ZiUTx(i)(17)\n",
            "In Eq. 17, ci^ZT\n",
            "i=a(t)^VT\n",
            "ci^Wi.As one can observe from\n",
            "Eq\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:21.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 14\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:21.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many layers are in the half-moon neural network?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:21.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne\n",
            "can clearly interpret and moreover make deduction from the\n",
            "decision tree, some of which are as f\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:22.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:22.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the main computational advantage of decision trees? -A: Less storage memory, -B: Fewer operations, -C: Lower accuracy\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:22.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThere are also\n",
            "some categories that emerged although none of the training\n",
            "data falls to them.\n",
            "Beside\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:23.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:23.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:24.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf to Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:24.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 143 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 5/5 [00:00<00:00,  8.77it/s]\n",
            "\u001b[32m2025-01-29 09:08:27.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:27.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: According to the guide, what is the typical license used to grant reuse rights with libre open access? -A: GNU General Public License -B: Creative Commons license -C: MIT license\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:27.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne of these options is open access.\n",
            "The basic idea of open access is that it makes \n",
            "copyrightable w\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([143, 508, 128])\n",
            "doc_masks: torch.Size([143, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:08:29.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:29.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: how many peer-reviewed open access journals are indexed by the Directory of Open Access Journals (DOAJ)? -A: Over 10,000 -B: Over 20,000 -C: Exactly 30,000\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:29.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mFor authors of articles, a good \n",
            "place to start is the Directory of Open Access Journals (“DOAJ”), a\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:30.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:30.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Does open access eliminate price barriers?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:30.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOne of these options is open access.\n",
            "The basic idea of open access is that it makes \n",
            "copyrightable w\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:32.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:32.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Are publication fees required for all open access journals?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:32.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1morg/how-we-work/general-information/open-access-policy .\n",
            "4. For example, the Budapest Open Access In\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:33.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:33.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In what year did the Bill and Melinda Gates foundation implement an open access policy?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:33.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m22 \n",
            "Under the policy, many federal agencies are required to develop plans to make the published resu\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:35.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 2015\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:35.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Are Gold Open Access and Green Open Access mutually exclusive.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:35.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIf you opt to use the Green Open Access  \n",
            "model, you will then need to select the best online \n",
            "venue\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:36.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:36.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://commission.europa.eu/document/download/1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:37.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://commission.europa.eu/document/download/1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf to 1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:37.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 364 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 12/12 [00:01<00:00,  9.75it/s]\n",
            "\u001b[32m2025-01-29 09:08:44.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:08:44.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([364, 508, 128])\n",
            "doc_masks: torch.Size([364, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:09:44.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which type of water must be supplied in a toilet sink? -A: hot -B: cold -C: hot and cold\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:44.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWelfare facilities for areas other than offices (restaurants, k itchens, conference rooms, daycare \n",
            "\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:46.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:46.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In which type of parkings must a carbon monoxide detector be installed? -A: indoor -B: underground -C: indoor or underground\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:46.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mI.2.8.    GAS DETECTION AND VE NTING  \n",
            "The requirements outlined below must be met in addition to th\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:48.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:48.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What percentage is the daylight factor required for façades with exterior obstructions? -A: 0.7% -B: 80% -C: 0.77%\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:48.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m4.    VISUAL COMFORT  \n",
            "4.1.    Natural lighting  \n",
            "Natural light is required for all permanent wor k \u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:50.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:50.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What fire resistance must vertical partitions have? -A: EI30 -B: EI60 -C: EI90\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:50.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe fire -resistant properties of the coverings for the vertical partitions, ceilings, flooring or \n",
            "\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:53.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:53.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:53.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf to CUDA_C_Programming_Guide.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:09:53.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 1803 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 57/57 [00:10<00:00,  5.68it/s]\n",
            "\u001b[32m2025-01-29 09:10:21.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:21.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum number of threads within a thread block?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:21.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOn current GPUs, a thread block may contain up to 1024 threads.\n",
            "However, a kernel can be executed by\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([1803, 508, 128])\n",
            "doc_masks: torch.Size([1803, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:10:22.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 1024\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:22.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you identify a thread with a four-dimensional index?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:22.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis provides a natural way\n",
            "to invoke computation across the elements in a domain such as a vector, \u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:23.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:23.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In the offline compilation process using nvcc, what happens to the device code? -A: It is directly executed on the host CPU. -B:  It is transformed into assembly and/or binary form. -C:  It is ignored and not used in the final application.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:23.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis section gives an overview of nvcc workflow and command\n",
            "options. A complete description can be f\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:25.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:25.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What are the two ways the host code can be output after being processed by nvcc? -A: As executable machine code, or a interpreted scripting language file. -B: As C++ code for later compilation, or as object code directly. -C: As an encrypted file or in a platform specific assembly format.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:25.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe modified host code is output either as C++ code that is left to be compiled using another tool o\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:26.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:26.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the primary purpose of just-in-time (JIT) compilation? -A: To convert host code into device code for execution on the GPU. -B: To optimize the performance of host code before it is compiled. -C: To compile PTX code into binary code at runtime by the device driver.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:26.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis environment vari-\n",
            "able can be used to validate\n",
            "that PTX code is embedded in\n",
            "an application and \u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:28.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:10:28.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:28.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What happens to the compiled binary code after JIT compilation by the device driver? -A: It is cached for later use and to avoid recompilation. -B: It's directly interpreted and doesn't need to be cached. -C: It is deleted after use to save space.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:28.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe modified host code is output either as C++ code that is left to be compiled using another tool o\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:31.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:31.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: When are virtual addresses assigned to graph allocations? -A: At the moment the graph is executed in the GPU. -B: When the allocation is actually being used by the execution. -C: When the node is created.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:31.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile these virtual addresses\n",
            "are fixed for the lifetime of the allocation node, the allocation cont\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:32.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:32.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What do graph memory nodes represent in a CUDA graph? -A: Actions like allocating or freeing the memmory. -B: Code that executes on the CPU to allocate memory. -C: The control flow and branching within a graph.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:32.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mGraph memory nodes are\n",
            "only supported on driver versions 11.4 and newer.\n",
            "379\n",
            "CUDA C++ Programming Gu\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:34.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:34.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: When does a graph allocation's lifetime end? -A: Only when the application shuts down. -B: When the execution reaches the freeing graph node, `cudaFreeAsync()`, or `cudaFree()`. -C: Only when the graph is destroyed.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:34.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mWhile these virtual addresses\n",
            "are fixed for the lifetime of the allocation node, the allocation cont\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:35.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:35.901\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How must operations accessing graph memory be ordered within a graph? -A: Before the allocation node and after the freeing node. -B: After any previous GPU execution. -C: After the allocation node and before the freeing operation.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:35.925\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mA program must guarantee that operations accessing graph memory:\n",
            "▶are ordered after the allocation n\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:43.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:43.510\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the primary benefit of Lazy Loading? -A: It reduces memory overhead and saves initalization time. -B: It allows programs to load all kernels faster during initialization. -C: It makes CUDA programs easier to debug and optimize.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:43.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLazy Loading\n",
            "23.1. What is Lazy Loading?\n",
            "Lazy Loading delays loading of CUDA modules and kernels fro\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:44.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:44.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you enable lazy loading by setting the env var `CUDA_MODULE_DATA_LOADING`?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:44.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mLazy Loading\n",
            "23.1. What is Lazy Loading?\n",
            "Lazy Loading delays loading of CUDA modules and kernels fro\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:46.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:46.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:47.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689 to ?uri=OJ:L_202401689.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:47.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 754 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/24 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 24/24 [00:03<00:00,  7.94it/s]\n",
            "\u001b[32m2025-01-29 09:11:57.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:57.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: what is a requirement for datasets used in high-risk AI systems? -A: Exclusively open-source datasets -B: Datasets ensuring quality and diversity -C: Datasets not exceeding 1 GB in size\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:57.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn particular , data \n",
            "sets should take into account, to the extent required by their intended purpos\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([754, 508, 128])\n",
            "doc_masks: torch.Size([754, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:11:59.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:59.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the threshold, measured in floating point operations, that leads to a presumption that a general-purpose AI model has systemic risk? -A: 10^1 -B: 10^20 -C: 10^25\u001b[0m\n",
            "\u001b[32m2025-01-29 09:11:59.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe full range of capabilities in a model could be better \n",
            "understo od after its placing on the mark\u001b[0m\n",
            "\u001b[32m2025-01-29 09:12:00.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No specific threshold is mentioned in the document.\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:12:00.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:00.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What should providers of AI systems that generate synthetic content ensure? -A: That the content is not marked in any way. -B: That the outputs are marked in a machine-readable format and detectable as artificially generated or manipulated. -C: That there is no way to detect that the content is synthetic.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:00.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThis obligation shall not apply to AI systems author ised by law to \n",
            "detect , prevent, invest igate \u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:02.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:02.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How long does a market surveillance authority have to take appropriate measures after receiving notification of a serious incident? -A: 3 days -B: 7 days -C: 14 days\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:02.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m7. Upon receiving a notif ication related to a serious incident refer red to in Article 3, point (49\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:04.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:04.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum duration of testing in real-world conditions? -A: 3 months -B: 6 months, with a possible extension of an additional 6 months. -C: 12 months\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:04.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m(e)data collected and processed for the purpose of the testing in real world conditions shall be tra\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:05.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:05.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum fine for supplying incorrect, incomplete, or misleading information to notified bodies or national competent authorities? -A: 7,500,000 EUR or 1% of annual turnover, whichever is higher. -B: 5,000,000 EUR or 0.5 % of annual turnover, whichever is higher -C: 10,000,000 EUR or 5% of annual turnover, whichever is higher\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:05.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mOJ L, 12.7.2024 EN\n",
            "ELI: http://data.europa.eu/eli/reg/2024/1689/oj 115/144\n",
            "5. The supply of incor re\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:07.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:07.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: By what date should codes of practice be ready? -A: 2 May 2025 -B: 2 May 2024 -C: 2 August 2025\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:07.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThe AI Office shall assist in the assessment of available standards.\n",
            "9. Codes of practice shall be r\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:09.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:09.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the time period for a market surveillance authority to inform the Commission of a finding related to a non-compliant AI system? -A: 1 month -B: 2 months -C: Immediately\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:09.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m7. The marke t surveillance author ities other than the mark et surveillance author ity of the Membe\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:10.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:10.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How long after a high-risk AI system has been placed on the market or put into service must the authorized representative keep the technical documentation, EU declaration of conformity and certificates available for competent authorities? -A: 5 years -B: 10 years C: 15 years\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:10.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m4. Impor ters shall ensure that, while a high-r isk AI syste m is under their responsibility , stora\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:11.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:11.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How long is the term of office for a Member State representative on the European Artificial Intelligence Board? -A: 2 years, renewable once -B: 3 years, renewable once -C: 4 years, renewable once\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:11.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCHAPTER VII\n",
            "GOVERNANCE\n",
            "SECTION 1\n",
            "Gove rnance at Union level\n",
            "Article 64\n",
            "AI Office\n",
            "1. The Commission s\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:13.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"B\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:13.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:14.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf to 7DUME_EN01_Rules.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:14.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 17 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 1/1 [00:00<00:00, 13.61it/s]\n",
            "\u001b[32m2025-01-29 09:13:15.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:15.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many chapters does the game last?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:15.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m1\n",
            "Will you play as the Fellowship of the Ring to defend the free races and destroy the One Ring?  \n",
            "O\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([17, 508, 128])\n",
            "doc_masks: torch.Size([17, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:13:16.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:13:16.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:16.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory conditions are there?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:16.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:18.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:18.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many different races are there?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:18.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m• \n",
            " Re\n",
            "veal tokens to both players. There is no hidden information.Bonus spaces\n",
            "2 matching Race symb\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:20.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 7\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:20.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which player begins the game? -A: Sauron -B: The Fellowship -C: Other\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:20.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mImmediately place a Fortress pawn on the corresponding region of the central board and benefit  from\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:21.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:21.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you take a Chapter card and a Landmark tile on your same turn?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:21.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:23.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:23.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many goins does a player take when discarding a card during Chapter 3?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:23.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThen, play it in front of you or discard it.\n",
            "  Preparing a chapter\n",
            "At the start of each chapter (1, \u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:24.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 3\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:24.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: After taking a landmark tile, do you reveal a new tile and the end of your turn?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:24.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:25.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:25.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can a player pay coins to compensate for missing skill symbols in a Landmark Tile?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:25.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIn addition to its effect  (see page 5), it has a chaining  symbol \n",
            "2 .In chapter 2, you may play th\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:27.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:27.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If a player is missing 2 skill symbols, how many coins must they pay to the reserve?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:27.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, the additional cost of your first tile is 0 Coins.\n",
            "  Skills\n",
            "In order to play them, tiles \u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:28.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 2\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:28.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you use a symbol more than once per turn?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:28.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:30.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:14:30.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:30.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Which type of cards provide coins? -A: Gray -B: Yellow -C: Blue\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:30.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTherefore, the additional cost of your first tile is 0 Coins.\n",
            "  Skills\n",
            "In order to play them, tiles \u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:32.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"None of the above\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:32.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: During which chapter the purple cards become available? -A: Chapter 1 -B: Chapter 2 -C: Chapter 3\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:32.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:33.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"C\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:33.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If you place or move an unit and an enemy fortress is present, does it trigger a conflict?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:33.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIf one or mor e enemy Units are present: trigger a conflict. Each player removes one of their Units \u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:37.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:37.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:37.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:39.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:39.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: In how many regions do you need to be present to win the game?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:39.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mConquering Middle-earth\n",
            "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:40.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": 7\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:40.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/is_eotn_rulebook.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:41.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mDownloaded https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/is_eotn_rulebook.pdf to is_eotn_rulebook.pdf.pdf\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:41.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 48 documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 2/2 [00:00<00:00,  9.92it/s]\n",
            "\u001b[32m2025-01-29 09:15:43.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:43.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: What is the maximum number of cards a player may acquire during the lookout phase?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:43.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m13. At the beginning of the game, each player draws 5 cards from their \n",
            "Clan deck, looks through the\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes:\n",
            "encodings: torch.Size([48, 508, 128])\n",
            "doc_masks: torch.Size([48, 508])\n",
            "Documents encoded!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-01-29 09:15:44.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 4\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:44.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is there a limit to the number of cards a player may have in their hand?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:44.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNOTE 1:  There’s no limit to the number of cards a player may have \n",
            "in their hand.  \n",
            "NOTE 2:  If the\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:50.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:50.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you raid the locations of a player that has passed during the action phase?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:50.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mStarting with the First player and continuing clockwise, \n",
            "each player performs one action at a time.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:52.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:52.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many points in the scoreboard must be reached during the Action phase to trigger the final round?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:52.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>Discard any remaining, face-up Island cards and reveal new ones.\n",
            " >Pass the First player marker to \u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:53.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": 25\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:15:53.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:53.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can players conquer and pillage the same island during the expedition phase?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:53.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>There is no limit to the number, type, or order of \n",
            "actions a player may take during the Action pha\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:55.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:55.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Do you need a fish to conquer a distant island?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:55.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mRations are needed for the long journey!\n",
            "A player can choose to Pillage  a selected Island card with\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:57.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:57.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory points you get from each conquered island?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:57.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mEach \n",
            "action draws the clans closer to becoming the greatest empire! The \n",
            "game ends in the same roun\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:58.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"The document says that players gain VPs from Pillaging Islands but not how many victory points each conquered island gives\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:58.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Is there a cleanup phase in the final round?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:16:58.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mGAME FLOW\n",
            "Note for Imperial Settlers fans \n",
            "You cannot Spend 2 Workers  \n",
            "to get a Resource or a card.\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:00.400\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:00.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: How many victory points are granted by a built Field Location card that work as an upgrade?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:00.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mIMPORT ANT: Some Field Locations work only as upgrades. These Fields have \n",
            "the Resources on the righ\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:01.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"1\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:01.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Do you need a ship to be on the expedition board to use a card that allos to pillage or conquer right away?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:01.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mNOTE 2: Some abilities in the \n",
            "game have a ‘/’ divider between \n",
            "presented choices. This should be \n",
            "t\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:04.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "\"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:04.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can you use the raid action without a Raze token?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:04.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mThus allowing a player to play \n",
            "a single Boost card or build a single Field Location before resolvin\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:07.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"No\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:07.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:07.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
            "(Gold tokens assigned to cards are\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:09.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"Yes\"\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:09.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mQuestion: If player 1 has 30 Victory points and 4 workers and player 2 has 30 Victory points and 3 workers, who wins the game? -A: Player 1 -B: Player 2 -C: It's a tie\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:09.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
            "(Gold tokens assigned to cards are\u001b[0m\n",
            "\u001b[32m2025-01-29 09:17:12.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m{\n",
            "  \"answer\": \"A\"\n",
            "}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logger.info(\"Loading input data\")\n",
        "data = pd.read_csv(\"structured_qa.csv\")\n",
        "data[\"pred_answer\"] = [None] * len(data)\n",
        "data[\"pred_section\"] = [None] * len(data)\n",
        "for document_link, document_data in data.groupby(\"document\"):\n",
        "    logger.info(f\"Downloading document {document_link}\")\n",
        "    downloaded_document = Path(f\"{Path(document_link).name}.pdf\")\n",
        "    if not Path(downloaded_document).exists():\n",
        "        urlretrieve(document_link, downloaded_document)\n",
        "        logger.info(f\"Downloaded {document_link} to {downloaded_document}\")\n",
        "    else:\n",
        "        logger.info(f\"File {downloaded_document} already exists\")\n",
        "\n",
        "    answers, sections = process_document(downloaded_document, document_data, model)\n",
        "\n",
        "    for index in document_data.index:\n",
        "        data.loc[index, \"pred_answer\"] = str(answers[index]).upper()\n",
        "        data.loc[index, \"pred_section\"] = sections[index]\n",
        "\n",
        "data.to_csv(\"results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "mltqL7Bhq3m1",
        "outputId": "9fc0b64a-2b6c-4e05-9165-5b6b5bf52508"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 22,\n        \"max\": 83,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          22,\n          44,\n          66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://arxiv.org/pdf/2201.11903\",\n          \"https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf\",\n          \"https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Classification of general-purpose AI models as general-purpose AI models with systemic risk\",\n          \"3.2 Results\",\n          \"15.3. API Fundamentals\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"What is the threshold, measured in floating point operations, that leads to a presumption that a general-purpose AI model has systemic risk? -A: 10^1 -B: 10^20 -C: 10^25\",\n          \"How many random samples were examined to understand model performance?\",\n          \"How many victory points you get from each conquered island?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"C\",\n          \"100\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"NO SPECIFIC THRESHOLD IS MENTIONED IN THE DOCUMENT.\",\n          \"50\",\n          \"THE DOCUMENT SAYS THAT PLAYERS GAIN VPS FROM PILLAGING ISLANDS BUT NOT HOW MANY VICTORY POINTS EACH CONQUERED ISLAND GIVES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_section\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-24db7cae-14a2-4c50-b273-38ce68f95697\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>document</th>\n",
              "      <th>section</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>pred_answer</th>\n",
              "      <th>pred_section</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
              "      <td>Classification of general-purpose AI models as...</td>\n",
              "      <td>What is the threshold, measured in floating po...</td>\n",
              "      <td>C</td>\n",
              "      <td>NO SPECIFIC THRESHOLD IS MENTIONED IN THE DOCU...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>44</td>\n",
              "      <td>https://arxiv.org/pdf/2201.11903</td>\n",
              "      <td>3.2 Results</td>\n",
              "      <td>How many random samples were examined to under...</td>\n",
              "      <td>100</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>47</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>CARD AND TILE EFFECTS</td>\n",
              "      <td>How many different races are there?</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>52</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>CARD AND TILE COSTS</td>\n",
              "      <td>Can a player pay coins to compensate for missi...</td>\n",
              "      <td>YES</td>\n",
              "      <td>NO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>CARD AND TILE EFFECTS</td>\n",
              "      <td>Which type of cards provide coins? -A: Gray -B...</td>\n",
              "      <td>B</td>\n",
              "      <td>NONE OF THE ABOVE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
              "      <td>EXPEDITION PHASE</td>\n",
              "      <td>How many victory points you get from each conq...</td>\n",
              "      <td>1</td>\n",
              "      <td>THE DOCUMENT SAYS THAT PLAYERS GAIN VPS FROM P...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>83</td>\n",
              "      <td>https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...</td>\n",
              "      <td>15.3. API Fundamentals</td>\n",
              "      <td>When are virtual addresses assigned to graph a...</td>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24db7cae-14a2-4c50-b273-38ce68f95697')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24db7cae-14a2-4c50-b273-38ce68f95697 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24db7cae-14a2-4c50-b273-38ce68f95697');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-971af5b3-6587-46a6-bf70-75f80fd65a6b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-971af5b3-6587-46a6-bf70-75f80fd65a6b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-971af5b3-6587-46a6-bf70-75f80fd65a6b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Unnamed: 0                                           document  \\\n",
              "22          22  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
              "44          44                   https://arxiv.org/pdf/2201.11903   \n",
              "47          47  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "52          52  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "55          55  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "66          66  https://github.com/mozilla-ai/structured-qa/re...   \n",
              "83          83  https://docs.nvidia.com/cuda/pdf/CUDA_C_Progra...   \n",
              "\n",
              "                                              section  \\\n",
              "22  Classification of general-purpose AI models as...   \n",
              "44                                        3.2 Results   \n",
              "47                              CARD AND TILE EFFECTS   \n",
              "52                                CARD AND TILE COSTS   \n",
              "55                              CARD AND TILE EFFECTS   \n",
              "66                                   EXPEDITION PHASE   \n",
              "83                             15.3. API Fundamentals   \n",
              "\n",
              "                                             question answer  \\\n",
              "22  What is the threshold, measured in floating po...      C   \n",
              "44  How many random samples were examined to under...    100   \n",
              "47                How many different races are there?      6   \n",
              "52  Can a player pay coins to compensate for missi...    YES   \n",
              "55  Which type of cards provide coins? -A: Gray -B...      B   \n",
              "66  How many victory points you get from each conq...      1   \n",
              "83  When are virtual addresses assigned to graph a...      C   \n",
              "\n",
              "                                          pred_answer  pred_section  \n",
              "22  NO SPECIFIC THRESHOLD IS MENTIONED IN THE DOCU...           NaN  \n",
              "44                                                 50           NaN  \n",
              "47                                                  7           NaN  \n",
              "52                                                 NO           NaN  \n",
              "55                                  NONE OF THE ABOVE           NaN  \n",
              "66  THE DOCUMENT SAYS THAT PLAYERS GAIN VPS FROM P...           NaN  \n",
              "83                                                  A           NaN  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.read_csv(\"results.csv\")\n",
        "results.loc[results[\"answer\"] != results[\"pred_answer\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4z9XxXWq3m1",
        "outputId": "327b1131-3ed5-40ff-f68d-48b0727873c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9292929292929293"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy = sum(results[\"answer\"] == results[\"pred_answer\"]) / len(results)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UXg_TC7R28QI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004ad74940344b6eb376ae4cfc85f26b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024598891b4f46299dc20b5cfd714e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45ce30572c20425691ebdabe0696b0ec",
            "placeholder": "​",
            "style": "IPY_MODEL_667241a7a4e6442b9e32450dbcbb0f56",
            "value": "tokenizer.json: 100%"
          }
        },
        "066c98c9848e4e00b68d0e98ec6f3c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8939bbe84c24ff8ad43c8d996d29af2",
            "placeholder": "​",
            "style": "IPY_MODEL_9a8317a8c8754d4d8b513a7fb0366c8d",
            "value": "config.json: 100%"
          }
        },
        "0bad96f6403c4042a9ed7bb491c1b25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42220b511f14cd8b89f5071c0d216a4",
            "max": 1633,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46097609bd4b46fa94c27a5dcfe98a1a",
            "value": 1633
          }
        },
        "0bebf69871bb4d04a5329ecb32d64b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11c36278698f4a6e8f606811eaff2166": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13dd434100e747588f8be140f55305a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1601603b8da04598b2a3b1b6532b9de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "167d14dc1f3b42fe9f4d9cc2ec341363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c9e0ff5abab4e378959f47c5655f9f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3084de2764a48089029ffafa1087e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_420798f709e2420d81d7223c34ca442e",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 72.1kB/s]"
          }
        },
        "1e7fcaa0156d4f09af4bf24a8607e787": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e532f20ae6d4a5c90d5beba1518d3ee",
            "placeholder": "​",
            "style": "IPY_MODEL_50215024305b41c38aec0a3808b3bc84",
            "value": "artifact.metadata: 100%"
          }
        },
        "1eda4198a078469dbba236c3ed8654c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d67e8902244d87ad72120b9fb71284": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e7fcaa0156d4f09af4bf24a8607e787",
              "IPY_MODEL_0bad96f6403c4042a9ed7bb491c1b25d",
              "IPY_MODEL_1c9e0ff5abab4e378959f47c5655f9f7"
            ],
            "layout": "IPY_MODEL_dd0ddf2594eb42b4babe6eeaf6a59bbb"
          }
        },
        "24ae74e4073749fba785b660dac48f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_944a78e6adaf4e3a87551d0bd5a6fc75",
            "placeholder": "​",
            "style": "IPY_MODEL_8d7d0da8d2344625aeef3d1c452a9c68",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "266e8497e8b04e3fad5d23391960ed13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ec9d176d11451bb049b62c278a86ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d523b694174b7596944eeb86a553d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73d7ade0b58b41d1b1ac109026eeabc8",
              "IPY_MODEL_cc1c0fcd84b94a199612c3e7ccd906cd",
              "IPY_MODEL_5dbe5cc9d6e64e5cb62e7018a42e1f8e"
            ],
            "layout": "IPY_MODEL_56de5716ee0146158e399759aef55c41"
          }
        },
        "2a59d91e7621422ebda4fefca0ee6760": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38bd9b6cec8f42f1a9b2caca71478f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3919381f1ae247219c7e4378a5d2e1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd3d79c0262467296061f64606e57ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_024598891b4f46299dc20b5cfd714e0c",
              "IPY_MODEL_9846ac95a9864f6aad40bffcd1595c48",
              "IPY_MODEL_f7e1a279ca7a4576a67d600c6e0fcad6"
            ],
            "layout": "IPY_MODEL_1eda4198a078469dbba236c3ed8654c3"
          }
        },
        "420798f709e2420d81d7223c34ca442e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44b4d7daccdb46f19db7675c3a7d4f49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ce30572c20425691ebdabe0696b0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46097609bd4b46fa94c27a5dcfe98a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e15263fae0140299c6a55ce95f7bd43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f1165cdc7ef4701889d0e6de6ac9ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50215024305b41c38aec0a3808b3bc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54af3da7793c404fa8b4e1062185ea68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24ae74e4073749fba785b660dac48f4c",
              "IPY_MODEL_895f37ac364f4c1aa4b3089fa286fca3",
              "IPY_MODEL_f63e1751a94246888bf0426a2288cb36"
            ],
            "layout": "IPY_MODEL_90076a55ec674636b93c7b1d741ea374"
          }
        },
        "56de5716ee0146158e399759aef55c41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b034562b2354e70a27bc06f5fe674cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dbe5cc9d6e64e5cb62e7018a42e1f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5d5dcca8cb498d8eb982b1cc1273fc",
            "placeholder": "​",
            "style": "IPY_MODEL_f6e97fcb881443beaec839bd64530d2d",
            "value": " 438M/438M [00:02&lt;00:00, 248MB/s]"
          }
        },
        "5e532f20ae6d4a5c90d5beba1518d3ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f145f7ffcd540149cd775f01e3da418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f76cfc2d293d4b409e4fc8bfa805af96",
            "placeholder": "​",
            "style": "IPY_MODEL_167d14dc1f3b42fe9f4d9cc2ec341363",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "667241a7a4e6442b9e32450dbcbb0f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6766b3d159fd4c29b853f3ad44616429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3919381f1ae247219c7e4378a5d2e1ff",
            "max": 405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a59d91e7621422ebda4fefca0ee6760",
            "value": 405
          }
        },
        "67f985db0d7b41f7b15f135d6acb039e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d1348ebec44687ac2a9151d52b1e8f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae77818599cd4bc2ac761865e81c3f15",
            "value": 231508
          }
        },
        "6da62a5ad31940329f00748ad6eab4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b4d7daccdb46f19db7675c3a7d4f49",
            "placeholder": "​",
            "style": "IPY_MODEL_ee9dca0e0f2c49a49fb50b623818cda9",
            "value": " 232k/232k [00:00&lt;00:00, 1.77MB/s]"
          }
        },
        "73d7ade0b58b41d1b1ac109026eeabc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f05356467fa4c2ab321004efa06e9c9",
            "placeholder": "​",
            "style": "IPY_MODEL_ed08c56e20194dbca6732642fb4af466",
            "value": "model.safetensors: 100%"
          }
        },
        "747558448b5e40038b270a6a6f6af6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d4aa0529fb74e81a08cc12aeb243456": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e15263fae0140299c6a55ce95f7bd43",
            "placeholder": "​",
            "style": "IPY_MODEL_cacdb3a3a0e04ca3b744fb82a3dcc925",
            "value": "vocab.txt: 100%"
          }
        },
        "7f05356467fa4c2ab321004efa06e9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a8270d87ef4c9b80c46c5236c8292f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d4aa0529fb74e81a08cc12aeb243456",
              "IPY_MODEL_67f985db0d7b41f7b15f135d6acb039e",
              "IPY_MODEL_6da62a5ad31940329f00748ad6eab4da"
            ],
            "layout": "IPY_MODEL_27ec9d176d11451bb049b62c278a86ff"
          }
        },
        "81ee5fe4f8044ab9819b9f767c41826e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8209dde69d4147739c522342bfedcccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_066c98c9848e4e00b68d0e98ec6f3c1f",
              "IPY_MODEL_c88515f583bd469ca0d6ca54a812ca14",
              "IPY_MODEL_a47e31ce610b4dcf8ac934ec11aefc65"
            ],
            "layout": "IPY_MODEL_38bd9b6cec8f42f1a9b2caca71478f4b"
          }
        },
        "895f37ac364f4c1aa4b3089fa286fca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_747558448b5e40038b270a6a6f6af6f0",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ee5fe4f8044ab9819b9f767c41826e",
            "value": 112
          }
        },
        "8d18d3f17569471fade4a2df380a245c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d7d0da8d2344625aeef3d1c452a9c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90076a55ec674636b93c7b1d741ea374": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "944a78e6adaf4e3a87551d0bd5a6fc75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97d1348ebec44687ac2a9151d52b1e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9846ac95a9864f6aad40bffcd1595c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c36278698f4a6e8f606811eaff2166",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8a050cfb1164c1cbecb0a86bc555d9c",
            "value": 466081
          }
        },
        "9a8317a8c8754d4d8b513a7fb0366c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2a6f8043e9943c7a6ec9112ac3d33bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a42220b511f14cd8b89f5071c0d216a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47e31ce610b4dcf8ac934ec11aefc65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a6f8043e9943c7a6ec9112ac3d33bd",
            "placeholder": "​",
            "style": "IPY_MODEL_8d18d3f17569471fade4a2df380a245c",
            "value": " 743/743 [00:00&lt;00:00, 64.1kB/s]"
          }
        },
        "ae77818599cd4bc2ac761865e81c3f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be5d5dcca8cb498d8eb982b1cc1273fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c88515f583bd469ca0d6ca54a812ca14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea0ed18363ec4a86b0383e0b43d38ac7",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfb0d5f3c9ae46dc910d335a9215521a",
            "value": 743
          }
        },
        "c8939bbe84c24ff8ad43c8d996d29af2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a050cfb1164c1cbecb0a86bc555d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cacdb3a3a0e04ca3b744fb82a3dcc925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb69dbb882694ed3bab1a2b35e0df524": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1c0fcd84b94a199612c3e7ccd906cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_266e8497e8b04e3fad5d23391960ed13",
            "max": 438349816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bebf69871bb4d04a5329ecb32d64b06",
            "value": 438349816
          }
        },
        "d1b166882cef441c816a75b784b3dcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f145f7ffcd540149cd775f01e3da418",
              "IPY_MODEL_6766b3d159fd4c29b853f3ad44616429",
              "IPY_MODEL_e3ec24ca9f384b6e8a6b25f66c9a2872"
            ],
            "layout": "IPY_MODEL_5b034562b2354e70a27bc06f5fe674cd"
          }
        },
        "d44706bfc8494edc8f266d3a94ff16a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd0ddf2594eb42b4babe6eeaf6a59bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb0d5f3c9ae46dc910d335a9215521a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3084de2764a48089029ffafa1087e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ec24ca9f384b6e8a6b25f66c9a2872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d44706bfc8494edc8f266d3a94ff16a2",
            "placeholder": "​",
            "style": "IPY_MODEL_13dd434100e747588f8be140f55305a3",
            "value": " 405/405 [00:00&lt;00:00, 30.5kB/s]"
          }
        },
        "ea0ed18363ec4a86b0383e0b43d38ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed08c56e20194dbca6732642fb4af466": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9dca0e0f2c49a49fb50b623818cda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f63e1751a94246888bf0426a2288cb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1165cdc7ef4701889d0e6de6ac9ed1",
            "placeholder": "​",
            "style": "IPY_MODEL_1601603b8da04598b2a3b1b6532b9de9",
            "value": " 112/112 [00:00&lt;00:00, 8.30kB/s]"
          }
        },
        "f6e97fcb881443beaec839bd64530d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f76cfc2d293d4b409e4fc8bfa805af96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e1a279ca7a4576a67d600c6e0fcad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004ad74940344b6eb376ae4cfc85f26b",
            "placeholder": "​",
            "style": "IPY_MODEL_cb69dbb882694ed3bab1a2b35e0df524",
            "value": " 466k/466k [00:00&lt;00:00, 3.50MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
