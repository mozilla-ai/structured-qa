{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fcx4osZYq3mt"
   },
   "source": [
    "# Structured Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE32hJKeq3mv"
   },
   "source": [
    "Source code: https://github.com/mozilla-ai/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDIEL7SNq3mv"
   },
   "source": [
    "Docs: https://mozilla-ai.github.io/structured-qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OwS4mKRq3mv"
   },
   "source": [
    "## GPU Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FYZaTmnq3mw"
   },
   "source": [
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to `Edit`→`Notebook Settings`\n",
    "- Select T4 GPU from the Hardware Accelerator section\n",
    "- Click `Save` and accept.\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RsETkxfq3mw",
    "outputId": "172850ad-a72e-434e-9686-9060fa95e660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU not available\")\n",
    "else:\n",
    "    print(\"GPU is available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEgVEmSQq3mx"
   },
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1eAychVq3my",
    "outputId": "b152776c-81c3-487a-d804-09ef5fb75258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragatouille\n",
      "  Downloading ragatouille-0.0.8.post4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting colbert-ai==0.2.19 (from ragatouille)\n",
      "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting fast-pytorch-kmeans==0.2.0.1 (from ragatouille)\n",
      "  Downloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: langchain>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.14)\n",
      "Requirement already satisfied: langchain_core>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (0.3.29)\n",
      "Collecting llama-index>=0.7 (from ragatouille)\n",
      "  Downloading llama_index-0.12.13-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
      "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting sentence-transformers<3.0.0,>=2.2.2 (from ragatouille)\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting srsly==2.4.8 (from ragatouille)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.11/dist-packages (from ragatouille) (4.47.1)\n",
      "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
      "  Downloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Collecting datasets (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (3.1.0)\n",
      "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
      "Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.67.1)\n",
      "Collecting ujson (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\n",
      "Collecting pynvml (from fast-pytorch-kmeans==0.2.0.1->ragatouille)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (3.11.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (0.2.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->ragatouille) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core>=0.1.4->ragatouille) (4.12.2)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl.metadata (727 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.13 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_core-0.12.13-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_readers_file-0.4.4-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index>=0.7->ragatouille) (3.9.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.27.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (11.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->ragatouille) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->ragatouille) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.0->ragatouille) (1.18.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core>=0.1.4->ragatouille) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.0)\n",
      "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.59.6)\n",
      "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille) (1.2.15)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille) (1.6.0)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille) (1.17.0)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_cloud-0.1.10-py3-none-any.whl.metadata (912 bytes)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (4.12.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.2.2)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_parse-0.5.20-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index>=0.7->ragatouille) (1.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->ragatouille) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.1.0->ragatouille) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->ragatouille) (3.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch>=1.13->ragatouille)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->ragatouille) (3.0.2)\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from git-python->colbert-ai==0.2.19->ragatouille) (3.1.44)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->fast-pytorch-kmeans==0.2.0.1->ragatouille)\n",
      "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.6)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain>=0.1.0->ragatouille) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.3.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.13->llama-index>=0.7->ragatouille)\n",
      "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->git-python->colbert-ai==0.2.19->ragatouille) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (2024.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index>=0.7->ragatouille) (1.17.0)\n",
      "Downloading ragatouille-0.0.8.post4-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl (8.8 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index-0.12.13-py3-none-any.whl (6.9 kB)\n",
      "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading voyager-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_agent_openai-0.4.2-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.13-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_llms_openai-0.3.14-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.4.2-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.4-py3-none-any.whl (39 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_cloud-0.1.10-py3-none-any.whl (247 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.0/248.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.5.20-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: colbert-ai\n",
      "  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114759 sha256=338c5f895f655f35f3dbcc0a7a946dacaa589dd2f089863452f6a6160a178d08\n",
      "  Stored in directory: /root/.cache/pip/wheels/14/75/5f/9680ae93eb0258ccf3e9d8cd34f328c53f8888c06c37067f3a\n",
      "Successfully built colbert-ai\n",
      "Installing collected packages: striprtf, nvidia-ml-py, filetype, dirtyjson, bitarray, xxhash, voyager, ujson, srsly, python-dotenv, pypdf, pynvml, onnx, ninja, mypy-extensions, marshmallow, fsspec, faiss-cpu, dill, typing-inspect, tiktoken, multiprocess, llama-cloud, git-python, dataclasses-json, llama-index-core, fast-pytorch-kmeans, datasets, sentence-transformers, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, colbert-ai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 2.5.0\n",
      "    Uninstalling srsly-2.5.0:\n",
      "      Successfully uninstalled srsly-2.5.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.1\n",
      "    Uninstalling sentence-transformers-3.3.1:\n",
      "      Successfully uninstalled sentence-transformers-3.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitarray-3.0.0 colbert-ai-0.2.19 dataclasses-json-0.6.7 datasets-3.2.0 dill-0.3.8 dirtyjson-1.0.8 faiss-cpu-1.9.0.post1 fast-pytorch-kmeans-0.2.0.1 filetype-1.2.0 fsspec-2024.9.0 git-python-1.0.3 llama-cloud-0.1.10 llama-index-0.12.13 llama-index-agent-openai-0.4.2 llama-index-cli-0.4.0 llama-index-core-0.12.13 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.4 llama-index-llms-openai-0.3.14 llama-index-multi-modal-llms-openai-0.4.2 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.4 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.20 marshmallow-3.26.0 multiprocess-0.70.16 mypy-extensions-1.0.0 ninja-1.11.1.3 nvidia-ml-py-12.560.30 onnx-1.17.0 pynvml-12.0.0 pypdf-5.1.0 python-dotenv-1.0.1 ragatouille-0.0.8.post4 sentence-transformers-2.7.0 srsly-2.4.8 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0 ujson-5.10.0 voyager-2.1.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "%pip install ragatouille PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I0dl5xGnq3my",
    "outputId": "68c881e3-6208-4748-f71b-f5a52b787108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark\n",
      "  Cloning https://github.com/mozilla-ai/structured-qa.git (to revision 5-add-benchmark) to /tmp/pip-req-build-49ruike5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mozilla-ai/structured-qa.git /tmp/pip-req-build-49ruike5\n",
      "  Running command git checkout -b 5-add-benchmark --track origin/5-add-benchmark\n",
      "  Switched to a new branch '5-add-benchmark'\n",
      "  Branch '5-add-benchmark' set up to track remote branch '5-add-benchmark' from 'origin'.\n",
      "  Resolved https://github.com/mozilla-ai/structured-qa.git to commit 17942ca192e0493c7c061e6f908cc2b945122ef6\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting fire (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev62+g17942ca) (0.27.1)\n",
      "Collecting llama-cpp-python (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading llama_cpp_python-0.3.6.tar.gz (66.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting loguru (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev62+g17942ca) (2.10.5)\n",
      "Collecting pymupdf4llm (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading pymupdf4llm-0.0.17-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from structured-qa==0.3.3.dev62+g17942ca) (6.0.2)\n",
      "Collecting streamlit (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting unsloth (from structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading unsloth-2025.1.6-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->structured-qa==0.3.3.dev62+g17942ca) (2.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python->structured-qa==0.3.3.dev62+g17942ca) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python->structured-qa==0.3.3.dev62+g17942ca) (3.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev62+g17942ca) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->structured-qa==0.3.3.dev62+g17942ca) (2.27.2)\n",
      "Collecting pymupdf>=1.24.10 (from pymupdf4llm->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (8.1.8)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (17.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (0.10.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->structured-qa==0.3.3.dev62+g17942ca) (6.3.3)\n",
      "Collecting unsloth_zoo>=2025.1.4 (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading unsloth_zoo-2025.1.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (2.5.1+cu121)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting bitsandbytes (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (3.1.0)\n",
      "Collecting tyro (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading tyro-0.9.13-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (4.47.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.45.1)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.2.1)\n",
      "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.14.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting hf_transfer (from unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.5.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (1.21.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (3.11.11)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev62+g17942ca) (4.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python->structured-qa==0.3.3.dev62+g17942ca) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->structured-qa==0.3.3.dev62+g17942ca) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (2.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.1.105)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth->structured-qa==0.3.3.dev62+g17942ca) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.21.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.1.4->unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth->structured-qa==0.3.3.dev62+g17942ca)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth->structured-qa==0.3.3.dev62+g17942ca) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth->structured-qa==0.3.3.dev62+g17942ca) (1.18.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->structured-qa==0.3.3.dev62+g17942ca) (5.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (0.22.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->structured-qa==0.3.3.dev62+g17942ca) (1.17.0)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymupdf4llm-0.0.17-py3-none-any.whl (26 kB)\n",
      "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2025.1.6-py3-none-any.whl (175 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.1.5-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.13-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: structured-qa, fire, llama-cpp-python\n",
      "  Building wheel for structured-qa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for structured-qa: filename=structured_qa-0.3.3.dev62+g17942ca-py3-none-any.whl size=16254 sha256=4a483dde13b83e4423b427dc48638180c87ff1dca8cb35d4a09006ef2ca537d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-za51p1on/wheels/be/a2/66/5bd06ba07afee632d178971d710ae5150fe6379c43e361cd32\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=d9cddb798cd09136c441440f7a103ae7e5879184815ed603d5d49bd8a9e39570\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.6-cp311-cp311-linux_x86_64.whl size=4070578 sha256=6aa44ca69ab5b970dc8498c4f69366acbfa247fd7a03849742858bf7ca77d063\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/96/d2/acfb576f7a58ef0580e2fec8096e5eefd17cc356017089337b\n",
      "Successfully built structured-qa fire llama-cpp-python\n",
      "Installing collected packages: watchdog, shtab, pymupdf, protobuf, loguru, hf_transfer, fire, diskcache, pymupdf4llm, pydeck, llama-cpp-python, tyro, xformers, cut_cross_entropy, bitsandbytes, trl, streamlit, unsloth_zoo, unsloth, structured-qa\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.1 cut_cross_entropy-25.1.1 diskcache-5.6.3 fire-0.7.0 hf_transfer-0.1.9 llama-cpp-python-0.3.6 loguru-0.7.3 protobuf-3.20.3 pydeck-0.9.1 pymupdf-1.25.2 pymupdf4llm-0.0.17 shtab-1.7.1 streamlit-1.41.1 structured-qa-0.3.3.dev62+g17942ca trl-0.13.0 tyro-0.9.13 unsloth-2025.1.6 unsloth_zoo-2025.1.5 watchdog-6.0.0 xformers-0.0.29.post1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "a56f4501de384e0e8c5cf504d1337657",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install git+https://github.com/mozilla-ai/structured-qa.git@5-add-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl_haxghq3mz",
    "outputId": "18bc7cbf-feaa-481d-9d84-6c8390ff258d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-24 10:32:07--  https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14711 (14K) [text/plain]\n",
      "Saving to: ‘structured_qa.csv’\n",
      "\n",
      "\rstructured_qa.csv     0%[                    ]       0  --.-KB/s               \rstructured_qa.csv   100%[===================>]  14.37K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-24 10:32:07 (73.1 MB/s) - ‘structured_qa.csv’ saved [14711/14711]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mozilla-ai/structured-qa/refs/heads/5-add-benchmark/benchmark/structured_qa.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdWx_e7iq3mz"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vGqX_bU5q3mz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.colab.userdata import get, SecretNotFoundError\n",
    "\n",
    "try:\n",
    "    genai.configure(api_key=get(\"GOOGLE_API_KEY\"))\n",
    "except SecretNotFoundError as e:\n",
    "    raise RuntimeError(\"Please set the GOOGLE_API_KEY secret to your API key\") from e\n",
    "os.environ[\"LOGURU_LEVEL\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cbkIjBYNq3mz"
   },
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BiUeBWnIq3mz"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "\n",
    "def load_pdf(pdf_file: str) -> str | None:\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        return \"\\n\".join(page.extract_text() for page in pdf_reader.pages)\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Process all questions for a single Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Ilxn8LGFq3m0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from ragatouille.data import CorpusProcessor\n",
    "\n",
    "\n",
    "def process_document(\n",
    "    document_file,\n",
    "    document_data,\n",
    "    model,\n",
    "):\n",
    "    logger.info(\"Setting up RAG\")\n",
    "    RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "    corpus_processor = CorpusProcessor()\n",
    "    documents = corpus_processor.process_corpus([load_pdf(document_file)])\n",
    "    RAG.encode([x[\"content\"] for x in documents])\n",
    "\n",
    "    logger.info(\"Predicting\")\n",
    "    answers = {}\n",
    "    sections = {}\n",
    "    for index, row in document_data.iterrows():\n",
    "        if model.n > 0 and model.n % 9 == 0:\n",
    "            logger.info(\"Waiting for 60 seconds\")\n",
    "            time.sleep(60)\n",
    "        question = row[\"question\"]\n",
    "\n",
    "        logger.info(f\"Question: {question}\")\n",
    "        results = RAG.search_encoded_docs(query=question, k=3)\n",
    "        current_info = \"\\n\".join(result[\"content\"] for result in results)\n",
    "        logger.info(current_info[:100])\n",
    "\n",
    "        answer = model.model.generate_content(\n",
    "            [f\"This is the document: {current_info}\", question]\n",
    "        )\n",
    "        logger.info(answer.text)\n",
    "        answers[index] = json.loads(answer.text)[\"answer\"]\n",
    "        sections[index] = None\n",
    "        model.n += 1\n",
    "\n",
    "    return answers, sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr3ke2aJq3m0"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "zKMHc0Ouq3m0"
   },
   "outputs": [],
   "source": [
    "from structured_qa.model_loaders import load_gemini_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "cMBl2dxLq3m0"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are given an input document and a question.\n",
    "You can only answer the question based on the information in the document.\n",
    "You will return a JSON name with a single key: \"answer\".\n",
    "In `\"answer\"`, you will return the answer using one of the following JSON types:\n",
    "- Yes/No (for boolean questions)\n",
    "Is the model an LLM?\n",
    "{\n",
    "  \"answer\": \"No\"\n",
    "}\n",
    "- Single number (for numeric questions)\n",
    "How many layers does the model have?\n",
    "{\n",
    "  \"answer\": 12\n",
    "}\n",
    "- Single letter (for multiple-choice questions)\n",
    "What is the activation function used in the model?\n",
    "-A: ReLU\n",
    "-B: Sigmoid\n",
    "-C: Tanh\n",
    "{\n",
    "  \"answer\": \"C\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "QV3pBXvhq3m0"
   },
   "outputs": [],
   "source": [
    "model = load_gemini_model(\n",
    "    \"gemini-2.0-flash-exp\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    generation_config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "    },\n",
    ")\n",
    "model.n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5jWlVBaq3m1"
   },
   "source": [
    "# Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W9r17Rz3q3m1",
    "outputId": "3232af63-09f7-4377-dff2-e8df725c1445"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:10:27.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mLoading input data\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:27.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/1706.03762\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:27.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 1706.03762.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:27.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 56 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "\u001b[32m2025-01-24 11:10:30.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:30.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What type of architecture does the model use? -A: decoder only -B: encoder only -C: encoder-decoder\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:30.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m(2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "increased the maximum output leng\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([56, 508, 128])\n",
      "doc_masks: torch.Size([56, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:10:31.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:31.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many layers compose the encoder?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:31.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAt each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additi\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:33.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 6\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:33.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many layers compose the decoder?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:33.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTo facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "laye\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:34.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 6\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:34.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many parallel attention heads are used?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:34.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmod\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:35.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 8\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:35.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does the final model use learned embeddings for the input and output tokens?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:35.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in o\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:36.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:36.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does the final model use learned positional embeddings?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:36.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTo this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:38.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:38.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many GPUs were used for training?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:38.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:39.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 8\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:39.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What type of GPUs were used for training? -A: NVIDIA A100 -B: NVIDIA P100 -C: NVIDIA T4\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:39.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:40.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:40.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What optimizer was used for training? -A: AdamW -B: Adam -C: SGD\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:40.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:42.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:10:42.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:42.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many warmup steps were used?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:42.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThe big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:43.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 4000\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:43.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What was the dropout rate used for the base model?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:43.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:45.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 0.1\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:45.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2106.09685v2.pdf\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:45.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 2106.09685v2.pdf.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:45.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 137 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.18it/s]\n",
      "\u001b[32m2025-01-24 11:11:53.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:53.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does LoRA work with any neural network containing dense layers?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:53.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mMore importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off betw\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([137, 508, 128])\n",
      "doc_masks: torch.Size([137, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:11:54.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:54.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How much memory is saved (in GB) when training GPT-3 175B with LoRA compared to full fine-tuning?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:54.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mOn GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\n",
      "350GB. With r= 4and only\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:56.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"850\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:56.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: By how much can LoRA reduce GPU memory requirements during training? -A: 10x, -B: 5x, -C: 3x\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:56.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mUsing GPT-3 175B as an example – deploying indepen-\n",
      "dent instances of ﬁne-tuned models, each with 17\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:57.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:57.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: In billions, how many trainable parameters does GPT-3 have?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:57.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThe results\n",
      "on WikiSQL have a ﬂuctuation around \u00060:5%, MNLI-m around \u00060:1%, and SAMSum around\n",
      "\u00060:2/\u0006\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:58.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": \"175\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:58.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does LoRA introduce additional inference latency compared to full fine-tuning?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:11:58.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mA Generalization of Full Fine-tuning. A more general form of ﬁne-tuning allows the training of\n",
      "a sub\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:00.611\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:00.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2201.11903\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:00.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 2201.11903.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:00.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 199 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.74it/s]\n",
      "\u001b[32m2025-01-24 11:12:03.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:03.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Is Arithmetic reasoning is a task that language models often find very easy?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:03.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([199, 508, 128])\n",
      "doc_masks: torch.Size([199, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:12:05.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:05.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many large language models were evaluated?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:05.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mFor AQuA, we used four exemplars\n",
      "and solutions from the training set, as given in Appendix Table 21.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:10.267\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": 5\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:12:10.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:10.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many benchmarks were used to evaluate arithmetic reasoning?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:10.289\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m3 Arithmetic Reasoning\n",
      "We begin by considering math word problems of the form in Figure 1, which mea\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:12.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 5\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:12.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Is symbolic reasoning usually simple for humans but challenging for language models?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:12.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m2We sample examples \u001460tokens to ﬁt into our input context window, and also limit the examples to \u00142\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:15.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:15.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many annotators provided independent chains of thought?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:15.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSimilar to the\n",
      "annotation process in Cobbe et al. (2021), annotators were not given speciﬁc instruct\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:18.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:18.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many random samples for examined to understand model errors?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:18.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mWe also randomly examined 50 random sam-\n",
      "ples for which the model gave the wrong answer.\n",
      "The summary\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:19.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 50\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:19.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Which symbolic reasoning task is used as an out-of-domain evaluation? -A: Coin Flip -B: Tower of Hanoi -C: Chess puzzles\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:19.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mPhoebe ﬂips the coin.\n",
      "Osvaldo does not ﬂip the coin. Is the coin still heads up?”\n",
      "!“no” ).\n",
      "As the co\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:21.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:21.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://arxiv.org/pdf/2210.05189\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:21.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 2210.05189.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:21.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 44 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.65it/s]\n",
      "\u001b[32m2025-01-24 11:13:22.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:22.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many layers are in the toy model (y = x^2)?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:22.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
      "1WTh(0)+tX\n",
      "i=1ci^ZiUTx(i)(17)\n",
      "In Eq. 17, ci^ZT\n",
      "i=a(t)^VT\n",
      "ci^Wi.As one can observe from\n",
      "Eq\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([44, 508, 128])\n",
      "doc_masks: torch.Size([44, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:13:23.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:23.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does the model use Sigmoid activation function?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:23.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mComputation and memory analysis of toy problems.\n",
      "3 layers with leaky-ReLU activations, except for la\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:25.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:25.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many parameters are in the y = x^2 toy model tree?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:25.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mo(t)=c1^ZT\n",
      "1WTh(0)+tX\n",
      "i=1ci^ZiUTx(i)(17)\n",
      "In Eq. 17, ci^ZT\n",
      "i=a(t)^VT\n",
      "ci^Wi.As one can observe from\n",
      "Eq\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:26.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 14\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:26.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can recurrent networks also be converted to decision trees?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:26.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTherefore, for continuous activations, the neural\n",
      "network equivalent tree immediately becomes inﬁnit\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:27.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:13:27.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:27.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many layers are in the half-moon neural network?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:27.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mOne\n",
      "can clearly interpret and moreover make deduction from the\n",
      "decision tree, some of which are as f\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:29.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:29.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the main computational advantage of decision trees? -A: Less storage memory, -B: Fewer operations, -C: Lower accuracy\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:29.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThere are also\n",
      "some categories that emerged although none of the training\n",
      "data falls to them.\n",
      "Beside\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:30.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:30.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://authorsalliance.org/wp-content/uploads/Documents/Guides/Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:30.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile Authors%20Alliance%20-%20Understanding%20Open%20Access.pdf.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:30.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 143 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.82it/s]\n",
      "\u001b[32m2025-01-24 11:14:33.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:33.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: According to the guide, what is the typical license used to grant reuse rights with libre open access? -A: GNU General Public License -B: Creative Commons license -C: MIT license\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:33.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCHAPTER 4: HOW \n",
      "“OPEN” DO YOU \n",
      "WANT TO MAKE \n",
      "YOUR WORK?\n",
      "UNDER CURRENT U.S. COPYRIGHT LAW, COPY-\n",
      "righ\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([143, 508, 128])\n",
      "doc_masks: torch.Size([143, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:14:34.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:34.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: how many peer-reviewed open access journals are indexed by the Directory of Open Access Journals (DOAJ)? -A: Over 10,000 -B: Over 20,000 -C: Exactly 30,000\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:34.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mFor authors of articles, a good \n",
      "place to start is the Directory of Open Access Journals (“DOAJ”), a\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:35.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:35.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: what is the term of office for members of the advisory board of the Authors Alliance? -A: The source does not specify a term of office for the advisory board. -B: 2 years -C: 4 years\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:35.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mWhen Ms. Salo was just \n",
      "beginning her career as a librarian, she was asked \n",
      "to write a chapter of a \u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:37.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:37.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Does open access eliminate price barriers?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mOne of these options is open access.\n",
      "The basic idea of open access is that it makes \n",
      "copyrightable w\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:38.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:38.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Are publication fees required for all open access journals?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:38.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1morg/how-we-work/general-information/open-access-policy .\n",
      "4. For example, the Budapest Open Access In\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:39.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:39.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: In what year did the Bill and Melinda Gates foundation implement an open access policy?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:39.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m22 \n",
      "Under the policy, many federal agencies are required to develop plans to make the published resu\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:40.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 2015\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:40.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Are Gold Open Access and Green Open Access mutually exclusive.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:40.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIf you opt to use the Green Open Access  \n",
      "model, you will then need to select the best online \n",
      "venue\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:42.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:42.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://commission.europa.eu/document/download/1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:42.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 1654ca52-ec72-4bae-ba40-d2fc0f3d71ae_en?filename=mit-1-performance-and-technical-performance-specification-v1-2_en.pdf.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:42.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 364 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 12/12 [00:01<00:00,  9.92it/s]\n",
      "\u001b[32m2025-01-24 11:14:48.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:14:48.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([364, 508, 128])\n",
      "doc_masks: torch.Size([364, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:15:48.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Which type of water must be supplied in a toilet sink? -A: hot -B: cold -C: hot and cold\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:48.943\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThis range may be amended to between 2 and 15  French degrees in the event that a specific \n",
      "demand e\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:50.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:50.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: In which type of parkings must a carbon monoxide detector be installed? -A: indoor -B: underground -C: indoor or underground\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:50.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mI.2.8.    GAS DETECTION AND VE NTING  \n",
      "The requirements outlined below must be met in addition to th\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:51.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:51.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the daylight factor required for façades with exterior obstructions?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:51.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m4.    VISUAL COMFORT  \n",
      "4.1.    Natural lighting  \n",
      "Natural light is required for all permanent wor k \u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:53.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"0.7\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:53.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What fire resistance must vertical partitions have? -A: EI30 -B: EI60 -C: EI90\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:53.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m- There must be no external windows, unless this is unavoidable for technical reasons.  \n",
      "Access cont\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:54.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:54.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:54.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile ?uri=OJ:L_202401689.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:15:54.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 754 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 24/24 [00:02<00:00,  8.24it/s]\n",
      "\u001b[32m2025-01-24 11:16:02.140\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:02.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Which type of AI systems are banned by the AI Act? -A: High-risk systems, -B: Manipulative systems, -C: Real-time biometric systems in public spaces\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:02.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1meducation and training and the context the AI systems are to be used in, and consider ing \n",
      "the perso\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([754, 508, 128])\n",
      "doc_masks: torch.Size([754, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:16:03.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:03.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: what is a requirement for datasets used in high-risk AI systems? -A: Exclusively open-source datasets -B: Datasets ensuring quality and diversity -C: Datasets not exceeding 1 GB in size\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:03.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIn particular , data \n",
      "sets should take into account, to the extent required by their intended purpos\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:04.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:04.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the threshold, measured in floating point operations, that leads to a presumption that a general-purpose AI model has systemic risk? -A: 10^1 -B: 10^20 -C: 10^25\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:04.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThe full range of capabilities in a model could be better \n",
      "understo od after its placing on the mark\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:06.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Not applicable\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:06.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What should providers of AI systems that generate synthetic content ensure? -A: That the content is not marked in any way. -B: That the outputs are marked in a machine-readable format and detectable as artificially generated or manipulated. -C: That there is no way to detect that the content is synthetic.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:06.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThis obligation shall not apply to AI systems author ised by law to \n",
      "detect , prevent, invest igate \u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:07.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:07.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How long does a market surveillance authority have to take appropriate measures after receiving notification of a serious incident? -A: 3 days -B: 7 days -C: 14 days\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:07.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m7. Upon receiving a notif ication related to a serious incident refer red to in Article 3, point (49\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:09.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:16:09.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:09.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the maximum duration of testing in real-world conditions? -A: 3 months -B: 6 months, with a possible extension of an additional 6 months. -C: 12 months\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:09.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m(e)data collected and processed for the purpose of the testing in real world conditions shall be tra\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:11.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:11.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the maximum fine for supplying incorrect, incomplete, or misleading information to notified bodies or national competent authorities? -A: 7,500,000 EUR or 1% of annual turnover, whichever is higher. -B: 5,000,000 EUR or 0.5 % of annual turnover, whichever is higher -C: 10,000,000 EUR or 5% of annual turnover, whichever is higher\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:11.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mOJ L, 12.7.2024 EN\n",
      "ELI: http://data.europa.eu/eli/reg/2024/1689/oj 115/144\n",
      "5. The supply of incor re\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:12.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:12.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: By what date should codes of practice be ready? -A: 2 May 2025 -B: 2 May 2024 -C: 2 August 2025\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:12.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThe AI Office shall assist in the assessment of available standards.\n",
      "9. Codes of practice shall be r\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:14.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"A\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:14.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the time period for a market surveillance authority to inform the Commission of a finding related to a non-compliant AI system? -A: 1 month -B: 2 months -C: Immediately\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:14.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m7. The marke t surveillance author ities other than the mark et surveillance author ity of the Membe\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:16.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:16.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How long after a high-risk AI system has been placed on the market or put into service must the authorized representative keep the technical documentation, EU declaration of conformity and certificates available for competent authorities? -A: 5 years -B: 10 years C: 15 years\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:16.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m4. Impor ters shall ensure that, while a high-r isk AI syste m is under their responsibility , stora\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:20.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:20.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How long is the term of office for a Member State representative on the European Artificial Intelligence Board? -A: 2 years, renewable once -B: 3 years, renewable once -C: 4 years, renewable once\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:20.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCHAPTER VII\n",
      "GOVERNANCE\n",
      "SECTION 1\n",
      "Gove rnance at Union level\n",
      "Article 64\n",
      "AI Office\n",
      "1. The Commission s\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:22.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:22.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:22.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile 7DUME_EN01_Rules.pdf.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:22.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 17 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "\u001b[32m2025-01-24 11:17:24.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:24.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many chapters does the game last?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:24.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m1\n",
      "Will you play as the Fellowship of the Ring to defend the free races and destroy the One Ring?  \n",
      "O\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([17, 508, 128])\n",
      "doc_masks: torch.Size([17, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:17:25.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:25.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many victory conditions are there?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:26.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mConquering Middle-earth\n",
      "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:27.345\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:27.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many different races are there?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:27.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m• \n",
      " Re\n",
      "veal tokens to both players. There is no hidden information.Bonus spaces\n",
      "2 matching Race symb\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:28.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 6\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:17:28.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:28.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Which player begins the game? -A: Sauron -B: The Fellowship -C: Other\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:28.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m2\n",
      "1\n",
      " 1\n",
      "2\n",
      "3\n",
      "3\n",
      "6\n",
      "7\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "8\n",
      "Fellowship  \n",
      "of the Ring \n",
      "play areaSauron \n",
      "play areaDiscard\n",
      "Central  \n",
      "pla\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:30.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:30.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can you take a Chapter card and a Landmark tile on your same turn?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:30.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:31.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:31.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many goins does a player take when discarding a card during Chapter 3?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:31.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThen, play it in front of you or discard it.\n",
      "  Preparing a chapter\n",
      "At the start of each chapter (1, \u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:32.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 3\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:32.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: After taking a landmark tile, do you reveal a new tile and the end of your turn?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:32.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSort the Alliance tokens according to their backs, to make one stack per Race. Shuffle each stack se\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:34.584\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:34.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can a player pay coins to compensate for missing skill symbols in a Landmark Tile?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:34.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIn addition to its effect  (see page 5), it has a chaining  symbol \n",
      "2 .In chapter 2, you may play th\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:36.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:36.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: If a player is missing 2 skill symbols, how many coins must they pay to the reserve?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:36.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mTherefore, the additional cost of your first tile is 0 Coins.\n",
      "  Skills\n",
      "In order to play them, tiles \u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:37.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 2\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:37.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can you use a symbol more than once per turn?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:37.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:39.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:39.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Which type of cards provide coins? -A: Gray -B: Yellow -C: Blue\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:39.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mMo\n",
      "ve 1 of your Units  \n",
      "to an adjacent region.\n",
      "Your opponent loses  \n",
      "1 Coin.\n",
      "Remove 1 enemy Unit  fr\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:40.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"B\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:40.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: During which chapter the purple cards become available? -A: Chapter 1 -B: Chapter 2 -C: Chapter 3\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:40.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mYou gain 1 Skill per symbol shown. Each symbol may only be used once per turn, on each of your turns\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:42.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"C\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:18:43.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:43.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: If you place or move an unit and an enemy fortress is present, does it trigger a conflict?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:43.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIf one or mor e enemy Units are present: trigger a conflict. Each player removes one of their Units \u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:44.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:44.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:44.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mConquering Middle-earth\n",
      "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:45.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:45.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: In how many regions do you need to be present to win the game?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:45.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mConquering Middle-earth\n",
      "If you are present in all 7 regions (with a Fortress and/or at least 1 Unit)\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:48.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 7\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:48.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mDownloading document https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/is_eotn_rulebook.pdf\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:48.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<cell line: 0>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mFile is_eotn_rulebook.pdf.pdf already exists\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:48.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSetting up RAG\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 48 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.41it/s]\n",
      "\u001b[32m2025-01-24 11:19:49.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mPredicting\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:49.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: What is the maximum number of cards a player may acquire during the lookout phase?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:49.729\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m13. At the beginning of the game, each player draws 5 cards from their \n",
      "Clan deck, looks through the\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "encodings: torch.Size([48, 508, 128])\n",
      "doc_masks: torch.Size([48, 508])\n",
      "Documents encoded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-24 11:19:51.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 4\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:51.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Is there a limit to the number of cards a player may have in their hand?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mNOTE 1:  There’s no limit to the number of cards a player may have \n",
      "in their hand.  \n",
      "NOTE 2:  If the\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:52.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:52.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can you raid the locations of a player that has passed during the action phase?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:52.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mStarting with the First player and continuing clockwise, \n",
      "each player performs one action at a time.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:53.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:53.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can players conquer and pillage the same island during the expedition phase?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:53.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m>There is no limit to the number, type, or order of \n",
      "actions a player may take during the Action pha\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:55.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:55.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many points in the scoreboard must be reached during the Action phase to trigger the final round?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:55.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m>Discard any remaining, face-up Island cards and reveal new ones.\n",
      " >Pass the First player marker to \u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:57.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": 25\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:57.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Is there a cleanup phase in the final round?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:57.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mGAME FLOW\n",
      "Note for Imperial Settlers fans \n",
      "You cannot Spend 2 Workers  \n",
      "to get a Resource or a card.\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:58.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:19:58.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mWaiting for 60 seconds\u001b[0m\n",
      "\u001b[32m2025-01-24 11:20:58.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: How many victory points are granted by a built Field Location card that work as an upgrade?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:20:58.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mIMPORT ANT: Some Field Locations work only as upgrades. These Fields have \n",
      "the Resources on the righ\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:00.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "\"answer\": 1\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:00.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can you use the raid action without a Raze token?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:00.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mThus allowing a player to play \n",
      "a single Boost card or build a single Field Location before resolvin\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:01.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"No\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:01.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:01.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
      "(Gold tokens assigned to cards are\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:07.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:07.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mQuestion: Can the game end in a tie?\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:07.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1m>add 1 Victory Point for every 1 Gold  remaining in their supply \n",
      "(Gold tokens assigned to cards are\u001b[0m\n",
      "\u001b[32m2025-01-24 11:21:08.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_document\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m{\n",
      "  \"answer\": \"Yes\"\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "logger.info(\"Loading input data\")\n",
    "data = pd.read_csv(\"structured_qa.csv\")\n",
    "data[\"pred_answer\"] = [None] * len(data)\n",
    "data[\"pred_section\"] = [None] * len(data)\n",
    "for document_link, document_data in data.groupby(\"document\"):\n",
    "    logger.info(f\"Downloading document {document_link}\")\n",
    "    downloaded_document = Path(f\"{Path(document_link).name}.pdf\")\n",
    "    if not Path(downloaded_document).exists():\n",
    "        urlretrieve(document_link, downloaded_document)\n",
    "        logger.info(f\"Downloaded {document_link} to {downloaded_document}\")\n",
    "    else:\n",
    "        logger.info(f\"File {downloaded_document} already exists\")\n",
    "\n",
    "    answers, sections = process_document(downloaded_document, document_data, model)\n",
    "\n",
    "    for index in document_data.index:\n",
    "        data.loc[index, \"pred_answer\"] = str(answers[index]).upper()\n",
    "        data.loc[index, \"pred_section\"] = sections[index]\n",
    "\n",
    "data.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "mltqL7Bhq3m1",
    "outputId": "54479b50-365a-4f5c-a06e-b6de90b773b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 12,\n        \"max\": 50,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          22,\n          50,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"https://arxiv.org/pdf/2210.05189\",\n          \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689\",\n          \"https://github.com/mozilla-ai/structured-qa/releases/download/0.3.2/7DUME_EN01_Rules.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Prohibited AI Practices (Article 5)\",\n          \"CHAPTER OVERVIEW\",\n          \"2.1 Fully Connected Networks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Which type of AI systems are banned by the AI Act? -A: High-risk systems, -B: Manipulative systems, -C: Real-time biometric systems in public spaces\",\n          \"Which player begins the game? -A: Sauron -B: The Fellowship -C: Other\",\n          \"Does the model use Sigmoid activation function?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"NO\",\n          \"C\",\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"B\",\n          \"C\",\n          \"YES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_section\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0a2d3144-eaf7-4b26-8d89-77e7af73d11e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>document</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>pred_section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>https://arxiv.org/pdf/2210.05189</td>\n",
       "      <td>2.1 Fully Connected Networks</td>\n",
       "      <td>Does the model use Sigmoid activation function?</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>Prohibited AI Practices (Article 5)</td>\n",
       "      <td>Which type of AI systems are banned by the AI ...</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>Classification rules (article 51)</td>\n",
       "      <td>What is the threshold, measured in floating po...</td>\n",
       "      <td>C</td>\n",
       "      <td>NOT APPLICABLE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>https://github.com/mozilla-ai/structured-qa/re...</td>\n",
       "      <td>CHAPTER OVERVIEW</td>\n",
       "      <td>Which player begins the game? -A: Sauron -B: T...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a2d3144-eaf7-4b26-8d89-77e7af73d11e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0a2d3144-eaf7-4b26-8d89-77e7af73d11e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0a2d3144-eaf7-4b26-8d89-77e7af73d11e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-02b7c35c-e276-41e3-8281-6761696417c3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02b7c35c-e276-41e3-8281-6761696417c3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-02b7c35c-e276-41e3-8281-6761696417c3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Unnamed: 0                                           document  \\\n",
       "12          12                   https://arxiv.org/pdf/2210.05189   \n",
       "22          22  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
       "24          24  https://eur-lex.europa.eu/legal-content/EN/TXT...   \n",
       "50          50  https://github.com/mozilla-ai/structured-qa/re...   \n",
       "\n",
       "                                section  \\\n",
       "12         2.1 Fully Connected Networks   \n",
       "22  Prohibited AI Practices (Article 5)   \n",
       "24    Classification rules (article 51)   \n",
       "50                     CHAPTER OVERVIEW   \n",
       "\n",
       "                                             question answer     pred_answer  \\\n",
       "12    Does the model use Sigmoid activation function?     NO             YES   \n",
       "22  Which type of AI systems are banned by the AI ...      C               B   \n",
       "24  What is the threshold, measured in floating po...      C  NOT APPLICABLE   \n",
       "50  Which player begins the game? -A: Sauron -B: T...      A               C   \n",
       "\n",
       "    pred_section  \n",
       "12           NaN  \n",
       "22           NaN  \n",
       "24           NaN  \n",
       "50           NaN  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"results.csv\")\n",
    "results.loc[results[\"answer\"] != results[\"pred_answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4z9XxXWq3m1",
    "outputId": "6acb2a06-aaa7-460f-b6cd-6b7bf87aa24e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(results[\"answer\"] == results[\"pred_answer\"]) / len(results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXg_TC7R28QI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
